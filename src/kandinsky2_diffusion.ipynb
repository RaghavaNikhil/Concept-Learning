{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhiram/miniconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/zt/x2ld76wd2v1c21_b4vvtfbm00000gn/T/ipykernel_22690/3885583381.py:27: FutureWarning: Importing `DiffusionPipeline` or `ImagePipelineOutput` from diffusers.pipeline_utils is deprecated. Please import from diffusers.pipelines.pipeline_utils instead.\n",
      "  from diffusers.pipeline_utils import DiffusionPipeline, ImagePipelineOutput\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "\n",
    "from diffusers import DDIMScheduler, KandinskyPipeline\n",
    "\n",
    "path = \"kandinsky-community/kandinsky-2-2\"\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import inspect\n",
    "from typing import Callable, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from diffusers.models import UNet2DConditionModel, VQModel\n",
    "from diffusers.schedulers import DDIMScheduler, DDPMScheduler\n",
    "from diffusers.configuration_utils import FrozenDict\n",
    "from diffusers.models import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers.pipeline_utils import DiffusionPipeline, ImagePipelineOutput\n",
    "# from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
    "\n",
    "from diffusers import KandinskyPriorPipeline, KandinskyPipeline, KandinskyV22PriorPipeline, KandinskyV22Pipeline\n",
    "from diffusers.utils import load_image\n",
    "import PIL\n",
    "from torchvision import transforms\n",
    "from diffusers.schedulers import DDIMScheduler,PNDMScheduler, LMSDiscreteScheduler\n",
    "from diffusers.utils import deprecate, logging\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_ddim(x_t, alpha_t: \"alpha_t\", alpha_tm1: \"alpha_{t-1}\", eps_xt):\n",
    "    \"\"\" from noise to image\"\"\"\n",
    "    return (\n",
    "        alpha_tm1**0.5\n",
    "        * (\n",
    "            (alpha_t**-0.5 - alpha_tm1**-0.5) * x_t\n",
    "            + ((1 / alpha_tm1 - 1) ** 0.5 - (1 / alpha_t - 1) ** 0.5) * eps_xt\n",
    "        )\n",
    "        + x_t\n",
    "    )\n",
    "\n",
    "def forward_ddim(x_t, alpha_t: \"alpha_t\", alpha_tp1: \"alpha_{t+1}\", eps_xt):\n",
    "    \"\"\" from image to noise, it's the same as backward_ddim\"\"\"\n",
    "    return backward_ddim(x_t, alpha_t, alpha_tp1, eps_xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_new_h_w(h, w, scale_factor=8):\n",
    "    new_h = h // scale_factor**2\n",
    "    if h % scale_factor**2 != 0:\n",
    "        new_h += 1\n",
    "    new_w = w // scale_factor**2\n",
    "    if w % scale_factor**2 != 0:\n",
    "        new_w += 1\n",
    "    return new_h * scale_factor, new_w * scale_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(pil_image, w=512, h=512):\n",
    "    pil_image = pil_image.resize((w, h), resample=Image.BICUBIC, reducing_gap=1)\n",
    "    arr = np.array(pil_image.convert(\"RGB\"))\n",
    "    arr = arr.astype(np.float16) / 127.5 - 1\n",
    "    arr = np.transpose(arr, [2, 0, 1])\n",
    "    image = torch.from_numpy(arr).unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "\n",
    "class NewKandinskyPipeline(KandinskyV22Pipeline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet: UNet2DConditionModel,\n",
    "        scheduler: Union[DDIMScheduler, DDPMScheduler],\n",
    "        movq: VQModel,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            movq=movq,\n",
    "        )\n",
    "        self.forward_diffusion = partial(self.backward_diffusion, reverse_process=True)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def get_image_latents(self, image, sample=False, rng_generator=None):\n",
    "        encoding_dist = self.movq.encode(image).latents\n",
    "        # if sample:\n",
    "        #     encoding = encoding_dist.sample(generator=rng_generator)\n",
    "        # else:\n",
    "        #     encoding = encoding_dist.mode()\n",
    "        latents = encoding_dist #* 0.18215\n",
    "        return latents\n",
    "\n",
    "    def get_timesteps(self, num_inference_steps, strength, device):\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "        t_start = max(num_inference_steps - init_timestep, 0)\n",
    "        timesteps = self.scheduler.timesteps[t_start:]\n",
    "\n",
    "        return timesteps, num_inference_steps - t_start\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def backward_diffusion(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]],\n",
    "        image_embeds: Union[torch.FloatTensor, List[torch.FloatTensor]],\n",
    "        negative_image_embeds: Union[torch.FloatTensor, List[torch.FloatTensor]],\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        height: int = 512,\n",
    "        width: int = 512,\n",
    "        num_inference_steps: int = 100,\n",
    "        guidance_scale: float = 4.0,\n",
    "        num_images_per_prompt: int = 1,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "        callback_steps: int = 1,\n",
    "        return_dict: bool = True,\n",
    "        reverse_process: bool = False,\n",
    "        strength:float =0.3,\n",
    "    ):\n",
    "\n",
    "        if isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        batch_size = batch_size * num_images_per_prompt\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "        # prompt_embeds, text_encoder_hidden_states, _ = self._encode_prompt(\n",
    "        #     prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "        # )\n",
    "\n",
    "        if isinstance(image_embeds, list):\n",
    "            image_embeds = torch.cat(image_embeds, dim=0)\n",
    "        if isinstance(negative_image_embeds, list):\n",
    "            negative_image_embeds = torch.cat(negative_image_embeds, dim=0)\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            image_embeds = image_embeds.repeat_interleave(num_images_per_prompt, dim=0)\n",
    "            negative_image_embeds = negative_image_embeds.repeat_interleave(num_images_per_prompt, dim=0)\n",
    "\n",
    "            image_embeds = torch.cat([negative_image_embeds, image_embeds], dim=0).to(\n",
    "                dtype=torch.float16, device=device\n",
    "            )\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps_tensor = self.scheduler.timesteps\n",
    "        timesteps_tensor, num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n",
    "\n",
    "        num_channels_latents = self.unet.config.in_channels\n",
    "\n",
    "        height, width = get_new_h_w(height, width, self.movq_scale_factor)\n",
    "\n",
    "        # create initial latent\n",
    "        latents = self.prepare_latents(\n",
    "            (batch_size, num_channels_latents, height, width),\n",
    "            torch.float16,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "            self.scheduler,\n",
    "        )\n",
    "\n",
    "        for i, t in enumerate(self.progress_bar(timesteps_tensor if not reverse_process else reversed(timesteps_tensor))):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "\n",
    "            added_cond_kwargs = {\"image_embeds\": image_embeds}\n",
    "            noise_pred = self.unet(\n",
    "                sample=latent_model_input,\n",
    "                timestep=t,\n",
    "                encoder_hidden_states=None,\n",
    "                added_cond_kwargs=added_cond_kwargs,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            if do_classifier_free_guidance:\n",
    "                noise_pred, variance_pred = noise_pred.split(latents.shape[1], dim=1)\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                _, variance_pred_text = variance_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "                noise_pred = torch.cat([noise_pred, variance_pred_text], dim=1)\n",
    "\n",
    "            if not (\n",
    "                hasattr(self.scheduler.config, \"variance_type\")\n",
    "                and self.scheduler.config.variance_type in [\"learned\", \"learned_range\"]\n",
    "            ):\n",
    "                noise_pred, _ = noise_pred.split(latents.shape[1], dim=1)\n",
    "\n",
    "            prev_timestep = (\n",
    "                t\n",
    "                - self.scheduler.config.num_train_timesteps\n",
    "                // self.scheduler.num_inference_steps\n",
    "            )\n",
    "            # ddim \n",
    "            alpha_prod_t = self.scheduler.alphas_cumprod[t]\n",
    "            alpha_prod_t_prev = (\n",
    "                self.scheduler.alphas_cumprod[prev_timestep]\n",
    "                if prev_timestep >= 0\n",
    "                else self.scheduler.final_alpha_cumprod\n",
    "            )\n",
    "            \n",
    "            if reverse_process:\n",
    "                alpha_prod_t, alpha_prod_t_prev = alpha_prod_t_prev, alpha_prod_t\n",
    "                latents = backward_ddim(\n",
    "                    x_t=latents,\n",
    "                    alpha_t=alpha_prod_t,\n",
    "                    alpha_tm1=alpha_prod_t_prev,\n",
    "                    eps_xt=noise_pred,\n",
    "                )\n",
    "            else:\n",
    "                latents = self.scheduler.step(\n",
    "                    noise_pred,\n",
    "                    t,\n",
    "                    latents,\n",
    "                    generator=generator,\n",
    "                ).prev_sample\n",
    "                \n",
    "\n",
    "            if callback is not None and i % callback_steps == 0:\n",
    "                step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                callback(step_idx, t, latents)\n",
    "        \n",
    "        return latents \n",
    "        \n",
    "    @torch.inference_mode()\n",
    "    def decode_image(self, latents: torch.FloatTensor, **kwargs) -> List[\"PIL_IMAGE\"]:\n",
    "        return self.movq.decode(latents, force_not_quantize=True)[\"sample\"]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def torch_to_numpy(self, image) -> List[\"PIL_IMAGE\"]:\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models import PriorTransformer\n",
    "from transformers import CLIPImageProcessor, CLIPTextModelWithProjection, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from diffusers.schedulers import UnCLIPScheduler\n",
    "from diffusers.utils import BaseOutput\n",
    "\n",
    "class KandinskyPriorPipelineOutput(BaseOutput):\n",
    "    \"\"\"\n",
    "    Output class for KandinskyPriorPipeline.\n",
    "\n",
    "    Args:\n",
    "        image_embeds (`torch.FloatTensor`)\n",
    "            clip image embeddings for text prompt\n",
    "        negative_image_embeds (`List[PIL.Image.Image]` or `np.ndarray`)\n",
    "            clip image embeddings for unconditional tokens\n",
    "    \"\"\"\n",
    "\n",
    "    image_embeds: Union[torch.FloatTensor, np.ndarray]\n",
    "    negative_image_embeds: Union[torch.FloatTensor, np.ndarray]\n",
    "    \n",
    "class NewKandinskyPriorPipeline(KandinskyV22PriorPipeline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        prior: PriorTransformer,\n",
    "        image_encoder: CLIPVisionModelWithProjection,\n",
    "        text_encoder: CLIPTextModelWithProjection,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        scheduler: UnCLIPScheduler,\n",
    "        image_processor: CLIPImageProcessor,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            prior=prior,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            scheduler=scheduler,\n",
    "            image_encoder=image_encoder,\n",
    "            image_processor=image_processor,\n",
    "        )\n",
    "\n",
    "    def _encode_image(\n",
    "        self,\n",
    "        image: Union[torch.Tensor, List[PIL.Image.Image]],\n",
    "        device,\n",
    "        num_images_per_prompt,\n",
    "    ):\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = self.image_processor(image, return_tensors=\"pt\").pixel_values.to(\n",
    "                dtype=self.image_encoder.dtype, device=device\n",
    "            )\n",
    "\n",
    "        image_emb = self.image_encoder(image)[\"image_embeds\"]  # B, D\n",
    "        image_emb = image_emb.repeat_interleave(num_images_per_prompt, dim=0)\n",
    "        image_emb.to(device=device)\n",
    "\n",
    "        return image_emb\n",
    "\n",
    "    def prepare_latents_new(self, emb, timestep, batch_size, num_images_per_prompt, dtype, device, generator=None):\n",
    "        emb = emb.to(device=device, dtype=dtype)\n",
    "\n",
    "        batch_size = batch_size * num_images_per_prompt\n",
    "\n",
    "        init_latents = emb\n",
    "\n",
    "        if batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] == 0:\n",
    "            additional_image_per_prompt = batch_size // init_latents.shape[0]\n",
    "            init_latents = torch.cat([init_latents] * additional_image_per_prompt, dim=0)\n",
    "        elif batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] != 0:\n",
    "            raise ValueError(\n",
    "                f\"Cannot duplicate `image` of batch size {init_latents.shape[0]} to {batch_size} text prompts.\"\n",
    "            )\n",
    "        else:\n",
    "            init_latents = torch.cat([init_latents], dim=0)\n",
    "\n",
    "        shape = init_latents.shape\n",
    "        noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
    "\n",
    "        # get latents\n",
    "        init_latents = self.scheduler.add_noise(init_latents, noise, timestep)\n",
    "        latents = init_latents\n",
    "\n",
    "        return latents\n",
    "\n",
    "    def get_timesteps(self, num_inference_steps, strength, device):\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "        t_start = max(num_inference_steps - init_timestep, 0)\n",
    "        timesteps = self.scheduler.timesteps[t_start:]\n",
    "\n",
    "        return timesteps, num_inference_steps - t_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def new_forward(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]],\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: int = 1,\n",
    "        num_inference_steps: int = 25,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        guidance_scale: float = 4.0,\n",
    "        output_type: Optional[str] = \"pt\",\n",
    "        return_dict: bool = True,\n",
    "        image_pil = None,\n",
    "        strength: float = 0.3,\n",
    "    ):\n",
    "\n",
    "        if isinstance(prompt, str):\n",
    "            prompt = [prompt]\n",
    "        elif not isinstance(prompt, list):\n",
    "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
    "\n",
    "        if isinstance(negative_prompt, str):\n",
    "            negative_prompt = [negative_prompt]\n",
    "        elif not isinstance(negative_prompt, list) and negative_prompt is not None:\n",
    "            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n",
    "\n",
    "        # if the negative prompt is defined we double the batch size to\n",
    "        # directly retrieve the negative prompt embedding\n",
    "        if negative_prompt is not None:\n",
    "            prompt = prompt + negative_prompt\n",
    "            negative_prompt = 2 * negative_prompt\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        batch_size = len(prompt)\n",
    "        batch_size = batch_size * num_images_per_prompt\n",
    "\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "        prompt_embeds, text_encoder_hidden_states, text_mask = self._encode_prompt(\n",
    "            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "        )\n",
    "\n",
    "        image_embeds = self._encode_image(image_pil, device, num_images_per_prompt)\n",
    "\n",
    "        # prior\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        prior_timesteps_tensor = self.scheduler.timesteps\n",
    "\n",
    "        embedding_dim = self.prior.config.embedding_dim\n",
    "\n",
    "        latents = image_embeds\n",
    "        prior_timesteps_tensor, num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n",
    "        latent_timestep = prior_timesteps_tensor[:1].repeat(batch_size)\n",
    "        latents = self.prepare_latents_new(\n",
    "            latents,\n",
    "            latent_timestep,\n",
    "            batch_size // num_images_per_prompt,\n",
    "            num_images_per_prompt,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "        )\n",
    "\n",
    "        for i, t in enumerate(self.progress_bar(prior_timesteps_tensor)):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "\n",
    "            predicted_image_embedding = self.prior(\n",
    "                latent_model_input,\n",
    "                timestep=t,\n",
    "                proj_embedding=prompt_embeds,\n",
    "                encoder_hidden_states=text_encoder_hidden_states,\n",
    "                attention_mask=text_mask,\n",
    "            ).predicted_image_embedding\n",
    "\n",
    "            if do_classifier_free_guidance:\n",
    "                predicted_image_embedding_uncond, predicted_image_embedding_text = predicted_image_embedding.chunk(2)\n",
    "                predicted_image_embedding = predicted_image_embedding_uncond + guidance_scale * (\n",
    "                    predicted_image_embedding_text - predicted_image_embedding_uncond\n",
    "                )\n",
    "\n",
    "            if i + 1 == prior_timesteps_tensor.shape[0]:\n",
    "                prev_timestep = None\n",
    "            else:\n",
    "                prev_timestep = prior_timesteps_tensor[i + 1]\n",
    "\n",
    "            latents = self.scheduler.step(\n",
    "                predicted_image_embedding,\n",
    "                timestep=t,\n",
    "                sample=latents,\n",
    "                generator=generator,\n",
    "                prev_timestep=prev_timestep,\n",
    "            ).prev_sample\n",
    "\n",
    "        latents = self.prior.post_process_latents(latents)\n",
    "\n",
    "        image_embeddings = latents\n",
    "\n",
    "        # if negative prompt has been defined, we retrieve split the image embedding into two\n",
    "        if negative_prompt is None:\n",
    "            zero_embeds = self.get_zero_embed(latents.shape[0], device=latents.device)\n",
    "\n",
    "        else:\n",
    "            image_embeddings, zero_embeds = image_embeddings.chunk(2)\n",
    "\n",
    "            if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n",
    "                self.prior_hook.offload()\n",
    "\n",
    "        if output_type not in [\"pt\", \"np\"]:\n",
    "            raise ValueError(f\"Only the output types `pt` and `np` are supported not output_type={output_type}\")\n",
    "\n",
    "        if output_type == \"np\":\n",
    "            image_embeddings = image_embeddings.cpu().numpy()\n",
    "            zero_embeds = zero_embeds.cpu().numpy()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image_embeddings, zero_embeds)\n",
    "\n",
    "        return KandinskyPriorPipelineOutput(image_embeds=image_embeddings, negative_image_embeds=zero_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhiram/miniconda3/envs/nlp/lib/python3.10/site-packages/diffusers/configuration_utils.py:217: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "pipe = NewKandinskyPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16)\n",
    "pipe.scheduler = DDIMScheduler.from_config(\"kandinsky-community/kandinsky-2-2-decoder\", subfolder=\"scheduler\")\n",
    "pipe = pipe.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewKandinskyPriorPipeline {\n",
       "  \"_class_name\": \"NewKandinskyPriorPipeline\",\n",
       "  \"_diffusers_version\": \"0.18.2\",\n",
       "  \"image_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPVisionModelWithProjection\"\n",
       "  ],\n",
       "  \"image_processor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"prior\": [\n",
       "    \"diffusers\",\n",
       "    \"PriorTransformer\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"UnCLIPScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModelWithProjection\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_prior = NewKandinskyPriorPipeline.from_pretrained(\n",
    "    \"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe_prior.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path, target_size=512):\n",
    "    \"\"\"Load an image, resize and output -1..1\"\"\"\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    image = prepare_image(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModelWithProjection, CLIPVisionModelWithProjection, BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "import os\n",
    "\n",
    "\n",
    "class T2IModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(T2IModel, self).__init__()\n",
    "        self.text_model = BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased', \n",
    "        )\n",
    "        # self.text_model = CLIPTextModelWithProjection.from_pretrained(\n",
    "        #     \"kandinsky-community/kandinsky-2-2-prior\", subfolder=\"text_encoder\"\n",
    "        # )\n",
    "        self.vision_model = CLIPVisionModelWithProjection.from_pretrained(\n",
    "            \"kandinsky-community/kandinsky-2-2-prior\", subfolder=\"image_encoder\"\n",
    "        )\n",
    "        self.fc = nn.Linear(2560, 1280)\n",
    "\n",
    "    def initialize_optimizer(self):\n",
    "        params = (\n",
    "            list(self.fc.parameters())\n",
    "        )\n",
    "        optimizer = AdamW(params, lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, input_imgs, instructions):\n",
    "        text_embeds = self.text_model(instructions).text_embeds\n",
    "        vision_embeds = self.vision_model(input_imgs).image_embeds\n",
    "        x = torch.cat((vision_embeds, text_embeds), 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def output_embedding(self, target_images):\n",
    "        target_image_output = self.vision_model(target_images)\n",
    "        target_image_embeds = target_image_output.image_embeds\n",
    "        return target_image_embeds\n",
    "\n",
    "    def custom_loss(self, output_embeddings, target_embeddings):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        loss = mse_loss(output_embeddings, target_embeddings)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def save_model(self, output_dir=\"../model_save/\", filename=\"model_checkpoint.pt\"):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        file_path = output_dir + filename\n",
    "        print(\"Saving model to %s\" % file_path)\n",
    "\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "\n",
    "    def get_cos(self, input1, input2):\n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        similarity = cos(input1, input2)\n",
    "        avg = torch.sum(similarity) / len(similarity)\n",
    "        return avg\n",
    "\n",
    "    def metrics(self, input1, input2):\n",
    "        cos = self.get_cos(input1, input2)\n",
    "        return [cos]\n",
    "\n",
    "    def visualization(self, input_img, instruction, filename,negative_instruction = \"\"):\n",
    "        output_embeddings = self.forward(input_img, instruction)\n",
    "        neg_image_embed = self.forward(input_img, negative_instruction)\n",
    "\n",
    "        pipe = KandinskyV22Pipeline.from_pretrained(\n",
    "            \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16\n",
    "        )\n",
    "        pipe.to(device)\n",
    "        image = pipe(\n",
    "            image_embeds = output_embeddings,\n",
    "            negative_image_embeds = neg_image_embed,\n",
    "            height = 768,\n",
    "            width = 768,\n",
    "            num_inference_steps=100,\n",
    "        ).images\n",
    "\n",
    "        image[0].save(filename)\n",
    "\n",
    "\n",
    "model = T2IModel()\n",
    "model.to(device=device)\n",
    "\n",
    "import torch\n",
    "\n",
    "def load_model_from_checkpoint(model, checkpoint_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load a PyTorch model from a saved checkpoint.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model architecture (untrained).\n",
    "    - checkpoint_path (str): Path to the saved model checkpoint (.pth file).\n",
    "    - device (str): Device to which the model should be loaded ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Model populated with the loaded weights.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the model state dictionary from the specified path\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Move the model to the desired device\n",
    "    model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage\n",
    "loaded_model = load_model_from_checkpoint(model, 'magicbrush_kadinsky_imagewithinstruction_10epochs_full_v1.pth', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edited_image(img_path, alternate_prompt, prompt=\"\"):\n",
    "    img = load_img(img_path, 512).to(\"mps\", dtype=torch.float16)\n",
    "    image_latents = pipe.get_image_latents(img)\n",
    "    # image_emb_main, zero_image_emb_main = pipe_prior(prompt, generator=torch.Generator(device=\"mps\").manual_seed(2023)).to_tuple()\n",
    "    # image_emb_alternate, zero_image_emb_alternate = pipe_prior(alternate_prompt, generator=torch.Generator(device=\"mps\").manual_seed(2023)).to_tuple()\n",
    "\n",
    "    # image_emb_main, zero_image_emb_main = pipe_prior.new_forward(strength=0.6, prompt=prompt, image_pil=Image.open(img_path), generator=torch.Generator(device=\"mps\").manual_seed(2023)).to_tuple()\n",
    "    # image_emb_alternate, zero_image_emb_alternate = pipe_prior.new_forward(strength=0.6, prompt=alternate_prompt, image_pil=Image.open(img_path), generator=torch.Generator(device=\"mps\").manual_seed(2023)).to_tuple()\n",
    "\n",
    "    # zero_imge_emb_alternate -> embeddings of black image\n",
    "    # zero_image_emb_main -> embeddings of black image\n",
    "    # image_emb_main -> embeddings of input image\n",
    "    # iage_emb_alternate -> embeddings of input image + prompt\n",
    "    image_emb_alternate = loaded_model(image, prompt)\n",
    "    zero_image_emb_alternate = loaded_model(black_image) \n",
    "    zero_image_emb_main = zero_image_emd_alternate\n",
    "    image_emb_main = loaded_model(image, \"\")\n",
    "\n",
    "    reversed_latents = pipe.forward_diffusion(\n",
    "        \"\",\n",
    "        image_embeds=image_emb_main,\n",
    "        negative_image_embeds=zero_image_emb_main,\n",
    "        guidance_scale=1,\n",
    "        num_inference_steps=50,\n",
    "        latents=image_latents,\n",
    "        strength=0.5,\n",
    "    )\n",
    "\n",
    "    alternate_latents = pipe.backward_diffusion(\n",
    "        \"\",\n",
    "        image_embeds=image_emb_alternate,\n",
    "        negative_image_embeds=zero_image_emb_main,\n",
    "        guidance_scale=1,\n",
    "        num_inference_steps=50,\n",
    "        latents=reversed_latents,\n",
    "        strength=0.5,\n",
    "    )\n",
    "\n",
    "    x = pipe.decode_image(alternate_latents)\n",
    "    x = pipe.torch_to_numpy(x)\n",
    "    return pipe.numpy_to_pil(x)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:09<00:00,  1.64it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.97it/s]\n",
      "100%|██████████| 25/25 [00:25<00:00,  1.01s/it]\n",
      "100%|██████████| 25/25 [00:11<00:00,  2.22it/s]\n"
     ]
    }
   ],
   "source": [
    "impath = Path(\"../assets/test/example2.jpeg\").expanduser()\n",
    "get_edited_image(img_path=impath, alternate_prompt=\"That should be a truck and not a car.\").save('output1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.37it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.24it/s]\n",
      "100%|██████████| 25/25 [00:24<00:00,  1.03it/s]\n",
      "100%|██████████| 25/25 [00:11<00:00,  2.23it/s]\n"
     ]
    }
   ],
   "source": [
    "impath = Path(\"../assets/test/example1.jpeg\").expanduser()\n",
    "get_edited_image(img_path=impath, alternate_prompt=\"Add mask to anyone of the player's face.\").save('output2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.42it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.92it/s]\n",
      "100%|██████████| 25/25 [00:28<00:00,  1.12s/it]\n",
      "100%|██████████| 25/25 [00:11<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "impath = Path(\"../assets/test/example.JPG\").expanduser()\n",
    "get_edited_image(img_path=impath, alternate_prompt=\"Change the STOP sign to GO sign.\").save('output3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
