{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3dc52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    print(\"Using mps backend\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e58f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from diffusers import (\n",
    "    KandinskyV22Pipeline,\n",
    "    KandinskyV22PriorEmb2EmbPipeline,\n",
    "    KandinskyV22PriorPipeline,\n",
    ")\n",
    "from diffusers.utils import load_image\n",
    "from torchvision.transforms import ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "190b4ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T2IModel(\n",
       "  (text_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (vision_model): CLIPVisionModelWithProjection(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(257, 1664)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-47): 48 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              (v_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              (q_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              (out_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1664, out_features=8192, bias=True)\n",
       "              (fc2): Linear(in_features=8192, out_features=1664, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): Linear(in_features=1664, out_features=1280, bias=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=2048, out_features=1280, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CLIPTextModelWithProjection, CLIPVisionModelWithProjection, BertModel\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "import os\n",
    "\n",
    "\n",
    "class T2IModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(T2IModel, self).__init__()\n",
    "        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.vision_model = CLIPVisionModelWithProjection.from_pretrained(\n",
    "            \"kandinsky-community/kandinsky-2-2-prior\", subfolder=\"image_encoder\"\n",
    "        )\n",
    "        # Adjust the input features of the FC layer to the combined size of text and vision outputs\n",
    "        self.fc = nn.Linear(self.text_model.config.hidden_size + self.vision_model.config.projection_dim, 1280)\n",
    "\n",
    "\n",
    "    def initialize_optimizer(self):\n",
    "        params = (\n",
    "            list(self.fc.parameters())\n",
    "        )\n",
    "        optimizer = AdamW(params, lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, input_imgs, input_txt, attention_mask=None):\n",
    "        text_outputs = self.text_model(input_txt, attention_mask=attention_mask)\n",
    "        text_embeds = text_outputs.last_hidden_state[:, 0, :]  # Use the representation of the [CLS] token\n",
    "\n",
    "        vision_outputs = self.vision_model(input_imgs)\n",
    "        vision_embeds = vision_outputs.image_embeds\n",
    "\n",
    "        combined_embeds = torch.cat((vision_embeds, text_embeds), dim=1)\n",
    "        x = self.fc(combined_embeds)\n",
    "        return x\n",
    "\n",
    "    def output_embedding(self, target_images):\n",
    "        target_image_output = self.vision_model(target_images)\n",
    "        target_image_embeds = target_image_output.image_embeds\n",
    "        return target_image_embeds\n",
    "\n",
    "    def custom_loss(self, output_embeddings, target_embeddings):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        loss = mse_loss(output_embeddings, target_embeddings)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def save_model(self, output_dir=\"../model_save/\", filename=\"model_checkpoint.pt\"):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        file_path = output_dir + filename\n",
    "        print(\"Saving model to %s\" % file_path)\n",
    "\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "\n",
    "    def get_cos(self, input1, input2):\n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        similarity = cos(input1, input2)\n",
    "        avg = torch.sum(similarity) / len(similarity)\n",
    "        return avg\n",
    "\n",
    "    def metrics(self, input1, input2):\n",
    "        cos = self.get_cos(input1, input2)\n",
    "        return [cos]\n",
    "\n",
    "    def visualization(self, input_img, instruction, instruction_attention_mask, filename, negative_instruction=None, negative_instruction_attention_mask=None):\n",
    "        # Generate output embeddings with the provided attention mask\n",
    "        output_embeddings = self.forward(input_img, instruction, attention_mask=instruction_attention_mask)\n",
    "\n",
    "        # Handle the negative instruction if provided\n",
    "        neg_image_embed = None\n",
    "        if negative_instruction is not None and negative_instruction_attention_mask is not None:\n",
    "            neg_image_embed = self.forward(input_img, negative_instruction, attention_mask=negative_instruction_attention_mask)\n",
    "        else:\n",
    "            # If no negative instruction is provided, we'll use a tensor of zeros as a placeholder\n",
    "            neg_image_embed = torch.zeros_like(output_embeddings)\n",
    "\n",
    "        # Initialize the pipeline for the Kandinsky V2.2 decoder\n",
    "        pipe = KandinskyV22Pipeline.from_pretrained(\n",
    "            \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16\n",
    "        )\n",
    "        pipe.to(device)  # Make sure 'self.device' is defined in your model class\n",
    "\n",
    "        # Generate the image using the pipeline\n",
    "        image = pipe(\n",
    "            image_embeds=output_embeddings,\n",
    "            negative_image_embeds=neg_image_embed,\n",
    "            height=768,\n",
    "            width=768,\n",
    "            num_inference_steps=100,\n",
    "        ).images\n",
    "\n",
    "        # Save the generated image\n",
    "        image[0].save(filename)\n",
    "\n",
    "model = T2IModel()\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dad9bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = model.initialize_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f026c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_model_from_checkpoint(model, checkpoint_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load a PyTorch model from a saved checkpoint.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model architecture (untrained).\n",
    "    - checkpoint_path (str): Path to the saved model checkpoint (.pth file).\n",
    "    - device (str): Device to which the model should be loaded ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Model populated with the loaded weights.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the model state dictionary from the specified path\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Move the model to the desired device\n",
    "    model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage\n",
    "loaded_model = load_model_from_checkpoint(model, 'magicbrush_kadinsky_bert_imagewithinstruction_10epochs_full_v1.pth', device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cb80403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def test(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Run the model on test data and compute average loss and cosine similarity.\n",
    "\n",
    "    Args:\n",
    "    - model (T2IModel): Model to test.\n",
    "    - dataloader (DataLoader): DataLoader containing the test data.\n",
    "    - device (str): Device on which to run the model (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Average loss and average cosine similarity over the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Put the model in evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    avg_cos = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_progress = tqdm(dataloader, desc=\"Test Set Validation\", leave=False)\n",
    "        for batch in val_progress:\n",
    "            \n",
    "            # Convert textual input to the right format\n",
    "            instructions = batch[1]\n",
    "            if isinstance(instructions, torch.Tensor):\n",
    "                instructions = instructions.tolist()\n",
    "            if not isinstance(instructions[0], str):\n",
    "                instructions = [tokenizer.decode(text_input) for text_input in instructions]\n",
    "                \n",
    "            inputs = tokenizer(instructions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_masks = inputs['attention_mask'].to(device)\n",
    "\n",
    "            # Prepare image data\n",
    "            input_images = batch[0].to(device)\n",
    "            target_images = batch[2].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output_embeddings = model(input_images, input_ids, attention_mask=attention_masks)\n",
    "            target_img_embeddings = model.output_embedding(target_images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = model.custom_loss(output_embeddings, target_img_embeddings)\n",
    "            total_eval_loss += loss.item()\n",
    "            avg_cos += model.get_cos(output_embeddings, target_img_embeddings).item()\n",
    "\n",
    "            # Update the progress bar with the validation loss\n",
    "            val_progress.set_postfix({'validation_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "            \n",
    "            # After processing the batch:\n",
    "            del input_images, input_ids, attention_masks, target_images, output_embeddings, target_img_embeddings, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    avg_val_loss = total_eval_loss / len(dataloader)\n",
    "    print(\" Test Set Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "    avg_cos /= len(dataloader)\n",
    "    print(\" Test Set Validation cosine similarity: {0:.2f}\".format(avg_cos))\n",
    "    \n",
    "    return avg_val_loss, avg_cos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfb7a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "test_dataset = torch.load('test_dataset_kandisky_bert_magicbrush.pth')\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, sampler=RandomSampler(test_dataset), batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cb2d9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Set Validation Loss: 0.25\n",
      " Test Set Validation cosine similarity: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Assuming you have the device defined already\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "avg_loss, avg_cos_sim = test(loaded_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0e3d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_instruction_length(dataloader, tokenizer):\n",
    "    \"\"\"\n",
    "    Compute the maximum instruction length from the dataloader batches.\n",
    "\n",
    "    Args:\n",
    "    - dataloader (DataLoader): DataLoader containing your data.\n",
    "    - tokenizer: Tokenizer used to tokenize the instructions.\n",
    "\n",
    "    Returns:\n",
    "    - int: Maximum instruction length.\n",
    "    \"\"\"\n",
    "    max_len = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        instructions = batch[1]  # Assuming instructions are in position 1 in your batch\n",
    "        for instruction in instructions:\n",
    "            decoded_string = tokenizer.decode(instruction)\n",
    "            tokens = tokenizer.tokenize(decoded_string)\n",
    "            length = len(tokens)\n",
    "            if length > max_len:\n",
    "                max_len = length\n",
    "                \n",
    "    return max_len\n",
    "\n",
    "def custom_encode(instruction, max_len, tokenizer):\n",
    "    encoded_inst = tokenizer.encode_plus(\n",
    "    instruction,  # Sentence to encode.\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    max_length=max_len + 10,  # Pad & truncate all sentences.\n",
    "    pad_to_max_length=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",  # Return pytorch tensors\n",
    "    )\n",
    "    \n",
    "    return encoded_inst[\"input_ids\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d688a845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4819b650d6664b0d9e8bbce89d9fdb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a1647ff89c4849810184d71441d4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import BertTokenizer, CLIPProcessor\n",
    "import torch\n",
    "\n",
    "# Assuming 'device' is already defined as 'cuda' or 'cpu'\n",
    "file_suffix = \"44437\"\n",
    "img = Image.open(file_suffix + \"-input.png\")\n",
    "\n",
    "# Initialize the tokenizer and the processor\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Compute the max length of instructions in the dataloader\n",
    "max_len = compute_max_instruction_length(test_dataloader, tokenizer)\n",
    "\n",
    "# Process the image\n",
    "inputs = processor(images=img, return_tensors=\"pt\")\n",
    "input_image = inputs[\"pixel_values\"].to(device)\n",
    "\n",
    "# Prepare the text input with padding and attention mask\n",
    "instruction = \"put a face mask on one of the players\"\n",
    "encoded_instruction = custom_encode(instruction, max_len, tokenizer).to(device)\n",
    "instruction_attention_mask = torch.zeros(encoded_instruction.shape, dtype=torch.long).to(device)\n",
    "instruction_attention_mask[encoded_instruction != tokenizer.pad_token_id] = 1\n",
    "\n",
    "# If 'instruction_1' is an empty or alternative string, prepare it similarly\n",
    "instruction_1 = \"\"\n",
    "encoded_instruction_1 = custom_encode(instruction_1, max_len, tokenizer).to(device)\n",
    "instruction_1_attention_mask = torch.zeros(encoded_instruction_1.shape, dtype=torch.long).to(device)\n",
    "instruction_1_attention_mask[encoded_instruction_1 != tokenizer.pad_token_id] = 1\n",
    "\n",
    "# Visualize the output\n",
    "loaded_model.visualization(\n",
    "    input_img=input_image,\n",
    "    instruction=encoded_instruction,  \n",
    "    instruction_attention_mask=instruction_attention_mask,  \n",
    "    filename=file_suffix + \"-output_generated_bert.png\",\n",
    "    negative_instruction=encoded_instruction_1,  \n",
    "    negative_instruction_attention_mask=instruction_1_attention_mask\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_check = \"Change the stop sign to say \\\"GO\\\"\"\n",
    "str_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d7c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
