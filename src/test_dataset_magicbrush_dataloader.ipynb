{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    print(\"Using mps backend\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(directory, filename):\n",
    "    if os.path.isdir(directory):\n",
    "        files = os.listdir(directory)\n",
    "        if filename in files:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataLoaderWithInstruction():\n",
    "    \n",
    "    \"\"\"\n",
    "    A class for loading images and associated instructions from a given JSON file. The class also \n",
    "    prepares a DataLoader for the images and instructions which can be used to feed into models.\n",
    "\n",
    "    Attributes:\n",
    "    - directory (str): Directory where the JSON file resides.\n",
    "    - filename (str): Name of the JSON file to read from.\n",
    "    - batch_size (int): Size of each batch in the DataLoader.\n",
    "    - processor (AutoProcessor): Processor for the CLIP model.\n",
    "    - tokenizer (AutoTokenizer): Tokenizer for the CLIP model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, directory, filename, batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize the ImageDataLoaderWithInstruction with directory, filename, and batch_size.\n",
    "\n",
    "        Args:\n",
    "        - directory (str): Directory where the JSON file resides.\n",
    "        - filename (str): Name of the JSON file.\n",
    "        - batch_size (int, optional): Size of each batch in the DataLoader. Defaults to 32.\n",
    "        \"\"\"\n",
    "        self.directory = directory\n",
    "        self.filename = filename\n",
    "        self.batch_size = batch_size\n",
    "        self.processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    def load_json_from_directory(self):\n",
    "        \"\"\"\n",
    "        Load the JSON data from the specified directory and filename.\n",
    "\n",
    "        Returns:\n",
    "        - dict: Loaded JSON data if file exists, or an empty dictionary if not.\n",
    "        \"\"\"\n",
    "        if check(self.directory, self.filename):\n",
    "            with open(os.path.join(self.directory, self.filename), 'r') as json_file:\n",
    "                data = json.load(json_file)\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"'{self.filename}' does not exist in the specified directory.\")\n",
    "            return {}\n",
    "    \n",
    "    def compute_max_instruction_length(self):\n",
    "        \"\"\"\n",
    "        Compute the maximum instruction length from the JSON data.\n",
    "\n",
    "        Returns:\n",
    "        - int: Maximum instruction length.\n",
    "        \"\"\"\n",
    "        max_len = 0\n",
    "        for item in self.json_data:\n",
    "            tokens = self.tokenizer.tokenize(item['instruction'])\n",
    "            length = len(tokens)\n",
    "            if length > max_len:\n",
    "                max_len = length\n",
    "        return max_len\n",
    "    \n",
    "    def load_images_from_json(self):\n",
    "        \"\"\"\n",
    "        Load images and their corresponding instructions from the JSON data. Images are processed \n",
    "        using the CLIP processor, and paths are constructed based on the JSON data.\n",
    "\n",
    "        Returns:\n",
    "        - dict: Dictionary with keys 'input', 'output', and 'instruction', containing processed images and instructions.\n",
    "        \"\"\"\n",
    "        image_data = {\n",
    "            'input': {}, \n",
    "            'output': {}, \n",
    "            'instruction': {}\n",
    "            }\n",
    "\n",
    "        for item in tqdm(self.json_data, desc=\"Processing images\"):\n",
    "            folder_name = item['input'].split('-')[0]\n",
    "            \n",
    "            input_image_path = os.path.join(self.directory, \"images\", folder_name, item['input'])\n",
    "            input_image = self.processor(images=Image.open(input_image_path), return_tensors=\"pt\")[\"pixel_values\"]\n",
    "            \n",
    "            output_image_path = os.path.join(self.directory, \"images\", folder_name, item['output'])\n",
    "            output_image = self.processor(images=Image.open(output_image_path), return_tensors=\"pt\")[\"pixel_values\"]\n",
    "            \n",
    "            instruction = item['instruction']\n",
    "            \n",
    "            image_data['input'][input_image_path] = input_image\n",
    "            image_data['output'][input_image_path] = output_image\n",
    "            image_data['instruction'][input_image_path] = instruction\n",
    "\n",
    "        return image_data\n",
    "    \n",
    "    def prepare_dataloader(self):\n",
    "        \"\"\"\n",
    "        Prepare a DataLoader using the images and instructions loaded from the JSON. Images are stored as tensors \n",
    "        and instructions are tokenized.\n",
    "\n",
    "        Returns:\n",
    "        - TensorDataset: Dataset containing input images, tokenized instructions, and output images.\n",
    "        - DataLoader: DataLoader built from the TensorDataset.\n",
    "        \"\"\"\n",
    "        test_input_imgs = []\n",
    "        test_output_imgs = []\n",
    "        input_ids = []\n",
    "        self.json_data = self.load_json_from_directory()\n",
    "        self.max_len = self.compute_max_instruction_length()\n",
    "        self.image_data = self.load_images_from_json()\n",
    "\n",
    "        for key in self.image_data['input'].keys():\n",
    "            test_input_imgs.append(self.image_data['input'][key])\n",
    "            test_output_imgs.append(self.image_data['output'][key])\n",
    "            \n",
    "            sent = self.image_data['instruction'][key]\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                sent,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len + 10,\n",
    "                pad_to_max_length=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            input_ids.append(encoded_dict[\"input_ids\"].squeeze(dim=0))\n",
    "        \n",
    "        test_input_imgs = torch.cat(test_input_imgs, dim=0)\n",
    "        test_output_imgs = torch.cat(test_output_imgs, dim=0)\n",
    "        input_ids = torch.stack(input_ids, dim=0)\n",
    "\n",
    "        test_dataset = TensorDataset(test_input_imgs, input_ids, test_output_imgs)\n",
    "        test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=self.batch_size)\n",
    "\n",
    "        return test_dataset, test_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'edit_turns.json' does not exist in the specified directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prati\\OneDrive\\Desktop\\Fall 23\\CSE 576 NLP\\Project\\NLP-fall-23-concept-edit-learning\\src\\test_dataset_magicbrush_dataloader.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prati/OneDrive/Desktop/Fall%2023/CSE%20576%20NLP/Project/NLP-fall-23-concept-edit-learning/src/test_dataset_magicbrush_dataloader.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39medit_turns.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prati/OneDrive/Desktop/Fall%2023/CSE%20576%20NLP/Project/NLP-fall-23-concept-edit-learning/src/test_dataset_magicbrush_dataloader.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test_loader_instance \u001b[39m=\u001b[39m ImageDataLoaderWithInstruction(directory_path, filename\u001b[39m=\u001b[39mfilename, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/prati/OneDrive/Desktop/Fall%2023/CSE%20576%20NLP/Project/NLP-fall-23-concept-edit-learning/src/test_dataset_magicbrush_dataloader.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test_dataset, test_dataloader \u001b[39m=\u001b[39m test_loader_instance\u001b[39m.\u001b[39;49mprepare_dataloader()\n",
      "\u001b[1;32mc:\\Users\\prati\\OneDrive\\Desktop\\Fall 23\\CSE 576 NLP\\Project\\NLP-fall-23-concept-edit-learning\\src\\test_dataset_magicbrush_dataloader.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prati/OneDrive/Desktop/Fall%2023/CSE%20576%20NLP/Project/NLP-fall-23-concept-edit-learning/src/test_dataset_magicbrush_dataloader.ipynb#W5sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     encoded_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prati/OneDrive/Desktop/Fall%2023/CSE%20576%20NLP/Project/NLP-fall-23-concept-edit-learning/src/test_dataset_magicbrush_dataloader.ipynb#W5sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m         sent,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prati/OneDrive/Desktop/Fall%2023/CSE%20576%20NLP/Project/NLP-fall-23-concept-edit-learning/src/test_dataset_magicbrush_dataloader.ipynb#W5sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prati/OneDrive/Desktop/Fall%2023/CSE%20576%20NLP/Project/NLP-fall-23-concept-edit-learning/src/test_dataset_magicbrush_dataloader.ipynb#W5sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prati/OneDrive/Desktop/Fall%2023/CSE%20576%20NLP/Project/NLP-fall-23-concept-edit-learning/src/test_dataset_magicbrush_dataloader.ipynb#W5sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prati/OneDrive/Desktop/Fall%2023/CSE%20576%20NLP/Project/NLP-fall-23-concept-edit-learning/src/test_dataset_magicbrush_dataloader.ipynb#W5sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m     input_ids\u001b[39m.\u001b[39mappend(encoded_dict[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/prati/OneDrive/Desktop/Fall%2023/CSE%20576%20NLP/Project/NLP-fall-23-concept-edit-learning/src/test_dataset_magicbrush_dataloader.ipynb#W5sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m test_input_imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(test_input_imgs, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prati/OneDrive/Desktop/Fall%2023/CSE%20576%20NLP/Project/NLP-fall-23-concept-edit-learning/src/test_dataset_magicbrush_dataloader.ipynb#W5sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m test_output_imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(test_output_imgs, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/prati/OneDrive/Desktop/Fall%2023/CSE%20576%20NLP/Project/NLP-fall-23-concept-edit-learning/src/test_dataset_magicbrush_dataloader.ipynb#W5sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(input_ids, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "directory_path = \"test\"\n",
    "filename = \"edit_turns.json\"\n",
    "test_loader_instance = ImageDataLoaderWithInstruction(directory_path, filename=filename, batch_size=32)\n",
    "test_dataset, test_dataloader = test_loader_instance.prepare_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataloader_components(dataset, batch_size, dataset_filename, params_filename):\n",
    "    \"\"\"\n",
    "    Save the TensorDataset and DataLoader parameters to disk.\n",
    "\n",
    "    Args:\n",
    "    - dataset (TensorDataset): The dataset you want to save.\n",
    "    - batch_size (int): Batch size for DataLoader.\n",
    "    - dataset_filename (str, optional): Name of the file to save the TensorDataset.\n",
    "    - params_filename (str, optional): Name of the file to save DataLoader parameters.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Save the TensorDataset\n",
    "    torch.save(dataset, dataset_filename)\n",
    "\n",
    "    # Save DataLoader-related parameters using a dictionary\n",
    "    dataloader_params = {\n",
    "        'batch_size': batch_size,\n",
    "        'shuffle': False,  # DataLoader is not shuffled since you use a sampler\n",
    "        'sampler': RandomSampler(dataset)  # You can just save the type of sampler as it's not stateful\n",
    "    }\n",
    "    torch.save(dataloader_params, params_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_filename = 'test_dataset_magicbrush.pth'\n",
    "test_params_filename = 'test_dataset_magicbrush_dataloader_params.pth'\n",
    "save_dataloader_components(dataset = test_dataset, \n",
    "                           batch_size = 32, \n",
    "                           dataset_filename = test_dataset_filename, \n",
    "                           params_filename = test_params_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
