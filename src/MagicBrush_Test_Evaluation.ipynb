{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3dc52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    print(\"Using mps backend\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e58f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from diffusers import (\n",
    "    KandinskyV22Pipeline,\n",
    "    KandinskyV22PriorEmb2EmbPipeline,\n",
    "    KandinskyV22PriorPipeline,\n",
    ")\n",
    "from diffusers.utils import load_image\n",
    "from torchvision.transforms import ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "190b4ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T2IModel(\n",
       "  (text_model): CLIPTextModelWithProjection(\n",
       "    (text_model): CLIPTextTransformer(\n",
       "      (embeddings): CLIPTextEmbeddings(\n",
       "        (token_embedding): Embedding(49408, 1280)\n",
       "        (position_embedding): Embedding(77, 1280)\n",
       "      )\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (text_projection): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "  )\n",
       "  (vision_model): CLIPVisionModelWithProjection(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(257, 1664)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-47): 48 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              (v_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              (q_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              (out_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1664, out_features=8192, bias=True)\n",
       "              (fc2): Linear(in_features=8192, out_features=1664, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): Linear(in_features=1664, out_features=1280, bias=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=2560, out_features=1280, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CLIPTextModelWithProjection, CLIPVisionModelWithProjection, BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "import os\n",
    "\n",
    "\n",
    "class T2IModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(T2IModel, self).__init__()\n",
    "        self.text_model = BertForSequenceClassification.from_pretrained(\n",
    "            'bert-base-uncased'\n",
    "        )\n",
    "        self.vision_model = CLIPVisionModelWithProjection.from_pretrained(\n",
    "            \"kandinsky-community/kandinsky-2-2-prior\", subfolder=\"image_encoder\"\n",
    "        )\n",
    "        self.fc = nn.Linear(2560, 1280)\n",
    "\n",
    "    def initialize_optimizer(self):\n",
    "        params = (\n",
    "            list(self.fc.parameters())\n",
    "        )\n",
    "        optimizer = AdamW(params, lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, input_imgs, instructions):\n",
    "        text_embeds = self.text_model(instructions).text_embeds\n",
    "        vision_embeds = self.vision_model(input_imgs).image_embeds\n",
    "        x = torch.cat((vision_embeds, text_embeds), 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def output_embedding(self, target_images):\n",
    "        target_image_output = self.vision_model(target_images)\n",
    "        target_image_embeds = target_image_output.image_embeds\n",
    "        return target_image_embeds\n",
    "\n",
    "    def custom_loss(self, output_embeddings, target_embeddings):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        loss = mse_loss(output_embeddings, target_embeddings)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def save_model(self, output_dir=\"../model_save/\", filename=\"model_checkpoint.pt\"):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        file_path = output_dir + filename\n",
    "        print(\"Saving model to %s\" % file_path)\n",
    "\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "\n",
    "    def get_cos(self, input1, input2):\n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        similarity = cos(input1, input2)\n",
    "        avg = torch.sum(similarity) / len(similarity)\n",
    "        return avg\n",
    "\n",
    "    def metrics(self, input1, input2):\n",
    "        cos = self.get_cos(input1, input2)\n",
    "        return [cos]\n",
    "\n",
    "    def visualization(self, input_img, instruction, filename,negative_instruction = \"\"):\n",
    "        output_embeddings = self.forward(input_img, instruction)\n",
    "        neg_image_embed = self.forward(input_img, negative_instruction)\n",
    "\n",
    "        pipe = KandinskyV22Pipeline.from_pretrained(\n",
    "            \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16\n",
    "        )\n",
    "        pipe.to(device)\n",
    "        image = pipe(\n",
    "            image_embeds = output_embeddings,\n",
    "            negative_image_embeds = neg_image_embed,\n",
    "            height = 768,\n",
    "            width = 768,\n",
    "            num_inference_steps=100,\n",
    "        ).images\n",
    "\n",
    "        image[0].save(filename)\n",
    "\n",
    "\n",
    "model = T2IModel()\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dad9bb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadibha2/.local/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = model.initialize_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f026c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_model_from_checkpoint(model, checkpoint_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load a PyTorch model from a saved checkpoint.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model architecture (untrained).\n",
    "    - checkpoint_path (str): Path to the saved model checkpoint (.pth file).\n",
    "    - device (str): Device to which the model should be loaded ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Model populated with the loaded weights.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the model state dictionary from the specified path\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Move the model to the desired device\n",
    "    model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage\n",
    "loaded_model = load_model_from_checkpoint(model, 'magicbrush_kadinsky_imagewithinstruction_10epochs_full_v1.pth', device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb80403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Run the model on test data and compute average loss and cosine similarity.\n",
    "\n",
    "    Args:\n",
    "    - model (T2IModel): Model to test.\n",
    "    - dataloader (DataLoader): DataLoader containing the test data.\n",
    "    - device (str): Device on which to run the model (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Average loss and average cosine similarity over the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Put the model in evaluation mode.\n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_loss = 0\n",
    "    avg_cos = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_progress = tqdm(dataloader, desc=\"Test Set Validation\", leave=False)\n",
    "        for batch in val_progress:\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            input_images = batch[0].to(device=device)\n",
    "            instructions = batch[1].to(device=device) \n",
    "            target_images = batch[2].to(device=device)\n",
    "            output_embeddings = model(input_images, instructions)\n",
    "            \n",
    "            # Generating target embeddings\n",
    "            target_img_embeddings = model.output_embedding(target_images.to(device=device))\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = model.custom_loss(output_embeddings, target_img_embeddings)\n",
    "            total_eval_loss += loss.item()\n",
    "            avg_cos += model.get_cos(output_embeddings, target_img_embeddings)\n",
    "\n",
    "            # Update the progress bar with the validation loss\n",
    "            val_progress.set_postfix({'validation_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "            \n",
    "            # After processing the batch:\n",
    "            del input_images, instructions, target_images, output_embeddings, target_img_embeddings, loss\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    avg_val_loss = total_eval_loss / len(dataloader)\n",
    "    print(\" Test Set Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "    avg_cos /= len(dataloader)\n",
    "    print(\" Test Set Validation cosine similarity: {0:.2f}\".format(avg_cos))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return avg_val_loss, avg_cos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb7a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "test_dataset = torch.load('test_dataset_magicbrush.pth')\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, sampler=RandomSampler(test_dataset), batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the device defined already\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "avg_loss, avg_cos_sim = test(loaded_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0e3d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_instruction_length(dataloader, tokenizer):\n",
    "    \"\"\"\n",
    "    Compute the maximum instruction length from the dataloader batches.\n",
    "\n",
    "    Args:\n",
    "    - dataloader (DataLoader): DataLoader containing your data.\n",
    "    - tokenizer: Tokenizer used to tokenize the instructions.\n",
    "\n",
    "    Returns:\n",
    "    - int: Maximum instruction length.\n",
    "    \"\"\"\n",
    "    max_len = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        instructions = batch[1]  # Assuming instructions are in position 1 in your batch\n",
    "        for instruction in instructions:\n",
    "            decoded_string = tokenizer.decode(instruction)\n",
    "            tokens = tokenizer.tokenize(decoded_string)\n",
    "            length = len(tokens)\n",
    "            if length > max_len:\n",
    "                max_len = length\n",
    "                \n",
    "    return max_len\n",
    "\n",
    "def custom_encode(instruction, max_len, tokenizer):\n",
    "    encoded_inst = tokenizer.encode_plus(\n",
    "    instruction,  # Sentence to encode.\n",
    "    add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "    max_length=max_len + 10,  # Pad & truncate all sentences.\n",
    "    pad_to_max_length=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",  # Return pytorch tensors\n",
    "    )\n",
    "    \n",
    "    return encoded_inst[\"input_ids\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d688a845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6fef45411145d6af5402ca94afe388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74957bcd2b844c084cc6a644d728738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "fileSuffix = \"6197\"\n",
    "img = Image.open(fileSuffix + \"-input.png\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "input_image = processor(images=img, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "max_len = compute_max_instruction_length(test_dataloader, tokenizer)\n",
    "\n",
    "instruction = \"It should be a bus not a truck\"\n",
    "instruction_1 = \"\"\n",
    "\n",
    "input_image = input_image.to(device)\n",
    "instruction = custom_encode(instruction,max_len,tokenizer).to(device)\n",
    "instruction_1 = custom_encode(instruction_1,max_len,tokenizer).to(device)\n",
    "\n",
    "model.visualization(input_image, instruction, fileSuffix + \"-output_generated.png\",instruction_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_check = \"Change the stop sign to say \\\"GO\\\"\"\n",
    "str_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d7c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
