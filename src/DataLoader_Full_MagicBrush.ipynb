{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b76f64b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40fcb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1614a5e",
   "metadata": {},
   "source": [
    "## ImageDataLoaderwithInstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7243802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(directory, filename):\n",
    "    if os.path.isdir(directory):\n",
    "        files = os.listdir(directory)\n",
    "        if filename in files:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class ImageDataLoaderWithInstruction():\n",
    "    \n",
    "    \"\"\"\n",
    "    A class for loading images and associated instructions from a given JSON file. The class also \n",
    "    prepares a DataLoader for the images and instructions which can be used to feed into models.\n",
    "\n",
    "    Attributes:\n",
    "    - directory (str): Directory where the JSON file resides.\n",
    "    - filename (str): Name of the JSON file to read from.\n",
    "    - batch_size (int): Size of each batch in the DataLoader.\n",
    "    - processor (AutoProcessor): Processor for the CLIP model or similar models.\n",
    "    - tokenizer (AutoTokenizer): Tokenizer for the CLIP model similar models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, directory, filename, processor, tokenizer, batch_size = 32):\n",
    "        \"\"\"\n",
    "        Initialize the ImageDataLoaderWithInstruction with directory, filename, processor, tokenizer, and batch_size.\n",
    "\n",
    "        Args:\n",
    "        - directory (str): Directory where the JSON file resides.\n",
    "        - filename (str): Name of the JSON file.\n",
    "        - processor (AutoProcessor): Processor for the CLIP model or similar models.\n",
    "        - tokenizer (AutoTokenizer): Tokenizer for the CLIP model or similar models.\n",
    "        - batch_size (int, optional): Size of each batch in the DataLoader. Defaults to 32.\n",
    "        \"\"\"\n",
    "        self.directory = directory\n",
    "        self.filename = filename\n",
    "        self.batch_size = batch_size\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def load_json_from_directory(self):\n",
    "        \"\"\"\n",
    "        Load the JSON data from the specified directory and filename.\n",
    "\n",
    "        Returns:\n",
    "        - dict: Loaded JSON data if file exists, or an empty dictionary if not.\n",
    "        \"\"\"\n",
    "        if check(self.directory, self.filename):\n",
    "            with open(os.path.join(self.directory, self.filename), 'r') as json_file:\n",
    "                data = json.load(json_file)\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"'{self.filename}' does not exist in the specified directory.\")\n",
    "            return {}\n",
    "    \n",
    "    def compute_max_instruction_length(self):\n",
    "        \"\"\"\n",
    "        Compute the maximum instruction length from the JSON data.\n",
    "\n",
    "        Returns:\n",
    "        - int: Maximum instruction length.\n",
    "        \"\"\"\n",
    "        max_len = 0\n",
    "        for item in self.json_data:\n",
    "            tokens = self.tokenizer.tokenize(item['instruction'])\n",
    "            length = len(tokens)\n",
    "            if length > max_len:\n",
    "                max_len = length\n",
    "        return max_len\n",
    "    \n",
    "    def load_images_from_json(self):\n",
    "        \"\"\"\n",
    "        Load images and their corresponding instructions from the JSON data. Images are processed \n",
    "        using the CLIP processor, and paths are constructed based on the JSON data.\n",
    "\n",
    "        Returns:\n",
    "        - dict: Dictionary with keys 'input', 'output', and 'instruction', containing processed images and instructions.\n",
    "        \"\"\"\n",
    "        image_data = {'input': {}, 'output': {}, 'instruction': {}}\n",
    "\n",
    "        for item in tqdm(self.json_data, desc=\"Processing images\"):\n",
    "            folder_name = item['input'].split('-')[0]\n",
    "            input_image_path = os.path.join(self.directory, \"images\", folder_name, item['input'])\n",
    "            \n",
    "            input_image = self.processor(images=Image.open(input_image_path), return_tensors=\"pt\")[\"pixel_values\"]\n",
    "            output_image_path = os.path.join(self.directory, \"images\", folder_name, item['output'])\n",
    "            output_image = self.processor(images=Image.open(output_image_path), return_tensors=\"pt\")[\"pixel_values\"]\n",
    "            \n",
    "            instruction = item['instruction']\n",
    "            \n",
    "            image_data['input'][input_image_path] = input_image\n",
    "            image_data['output'][input_image_path] = output_image\n",
    "            image_data['instruction'][input_image_path] = instruction\n",
    "\n",
    "        return image_data\n",
    "    \n",
    "    def prepare_dataloader(self):\n",
    "        \"\"\"\n",
    "        Prepare a DataLoader using the images and instructions loaded from the JSON. Images are stored as tensors \n",
    "        and instructions are tokenized.\n",
    "\n",
    "        Returns:\n",
    "        - TensorDataset: Dataset containing input images, tokenized instructions, and output images.\n",
    "        - DataLoader: DataLoader built from the TensorDataset.\n",
    "        \"\"\"\n",
    "        train_input_imgs = []\n",
    "        train_output_imgs = []\n",
    "        input_ids = []\n",
    "        self.json_data = self.load_json_from_directory()\n",
    "        self.max_len = self.compute_max_instruction_length()\n",
    "        self.image_data = self.load_images_from_json()\n",
    "\n",
    "        for key in self.image_data['input'].keys():\n",
    "            train_input_imgs.append(self.image_data['input'][key])\n",
    "            train_output_imgs.append(self.image_data['output'][key])\n",
    "            \n",
    "            sent = self.image_data['instruction'][key]\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                sent,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len + 10,\n",
    "                pad_to_max_length=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            input_ids.append(encoded_dict[\"input_ids\"].squeeze(dim=0))\n",
    "        \n",
    "        train_input_imgs = torch.cat(train_input_imgs, dim=0)\n",
    "        train_output_imgs = torch.cat(train_output_imgs, dim=0)\n",
    "        input_ids = torch.stack(input_ids, dim=0)\n",
    "\n",
    "        train_dataset = TensorDataset(train_input_imgs, input_ids, train_output_imgs)\n",
    "        \n",
    "        return train_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61dd2906",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ec588",
   "metadata": {},
   "source": [
    "## Train DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2372cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|                       | 2/8807 [00:00<07:33, 19.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|████████████████████| 8807/8807 [15:27<00:00,  9.50it/s]\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"train\"\n",
    "filename = \"edit_turns.json\"\n",
    "train_loader_instance = ImageDataLoaderWithInstruction(directory_path, filename, processor, tokenizer, batch_size=32)\n",
    "train_dataset= train_loader_instance.prepare_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7c47c",
   "metadata": {},
   "source": [
    "## Dev DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a7a5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████████████████| 528/528 [00:57<00:00,  9.23it/s]\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"dev\"\n",
    "filename = \"edit_turns.json\"\n",
    "dev_loader_instance = ImageDataLoaderWithInstruction(directory_path, filename, processor, tokenizer, batch_size=32)\n",
    "dev_dataset= dev_loader_instance.prepare_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8005b5f3",
   "metadata": {},
   "source": [
    "## Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eda9e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|████████████████████| 1053/1053 [00:45<00:00, 23.34it/s]\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"test\"\n",
    "filename = \"edit_turns.json\"\n",
    "test_loader_instance = ImageDataLoaderWithInstruction(directory_path, filename, processor, tokenizer, batch_size=32)\n",
    "test_dataset= test_loader_instance.prepare_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1b245",
   "metadata": {},
   "source": [
    "## Save Datasets and Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc4be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataloader_components(dataset, batch_size, dataset_filename):\n",
    "    \"\"\"\n",
    "    Save the TensorDataset and DataLoader parameters to disk.\n",
    "\n",
    "    Args:\n",
    "    - dataset (TensorDataset): The dataset you want to save.\n",
    "    - batch_size (int): Batch size for DataLoader.\n",
    "    - dataset_filename (str, optional): Name of the file to save the TensorDataset.\n",
    "    - params_filename (str, optional): Name of the file to save DataLoader parameters.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Save the TensorDataset\n",
    "    torch.save(dataset, dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b55681",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_filename = 'train_dataset_kandisky_bert_magicbrush.pth'\n",
    "save_dataloader_components(dataset = train_dataset, \n",
    "                           batch_size = 32, \n",
    "                           dataset_filename = train_dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6087eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset_filename = 'dev_dataset_kandisky_bert_magicbrush.pth'\n",
    "save_dataloader_components(dataset = dev_dataset, \n",
    "                           batch_size = 32, \n",
    "                           dataset_filename = dev_dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2768eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_filename = 'test_dataset_kandisky_bert_magicbrush.pth'\n",
    "save_dataloader_components(dataset = test_dataset, \n",
    "                           batch_size = 32, \n",
    "                           dataset_filename = test_dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a719f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
