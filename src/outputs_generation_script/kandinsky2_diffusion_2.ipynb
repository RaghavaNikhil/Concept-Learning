{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    print(\"Using mps backend\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkusumba/.conda/envs/nlp-project/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "\n",
    "from diffusers import DDIMScheduler, KandinskyPipeline\n",
    "\n",
    "path = \"kandinsky-community/kandinsky-2-2\"\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import inspect\n",
    "from typing import Callable, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from diffusers.models import UNet2DConditionModel, VQModel\n",
    "from diffusers.schedulers import DDIMScheduler, DDPMScheduler\n",
    "from diffusers.configuration_utils import FrozenDict\n",
    "from diffusers.models import AutoencoderKL, UNet2DConditionModel\n",
    "# from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
    "\n",
    "from diffusers import KandinskyPriorPipeline, KandinskyPipeline, KandinskyV22PriorPipeline, KandinskyV22Pipeline\n",
    "from diffusers.utils import load_image\n",
    "import PIL\n",
    "from torchvision import transforms\n",
    "from diffusers.schedulers import DDIMScheduler,PNDMScheduler, LMSDiscreteScheduler\n",
    "from diffusers.utils import deprecate, logging\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_ddim(x_t, alpha_t: \"alpha_t\", alpha_tm1: \"alpha_{t-1}\", eps_xt):\n",
    "    \"\"\" from noise to image\"\"\"\n",
    "    return (\n",
    "        alpha_tm1**0.5\n",
    "        * (\n",
    "            (alpha_t**-0.5 - alpha_tm1**-0.5) * x_t\n",
    "            + ((1 / alpha_tm1 - 1) ** 0.5 - (1 / alpha_t - 1) ** 0.5) * eps_xt\n",
    "        )\n",
    "        + x_t\n",
    "    )\n",
    "\n",
    "def forward_ddim(x_t, alpha_t: \"alpha_t\", alpha_tp1: \"alpha_{t+1}\", eps_xt):\n",
    "    \"\"\" from image to noise, it's the same as backward_ddim\"\"\"\n",
    "    return backward_ddim(x_t, alpha_t, alpha_tp1, eps_xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_new_h_w(h, w, scale_factor=8):\n",
    "    new_h = h // scale_factor**2\n",
    "    if h % scale_factor**2 != 0:\n",
    "        new_h += 1\n",
    "    new_w = w // scale_factor**2\n",
    "    if w % scale_factor**2 != 0:\n",
    "        new_w += 1\n",
    "    return new_h * scale_factor, new_w * scale_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(pil_image, w=512, h=512):\n",
    "    pil_image = pil_image.resize((w, h), resample=Image.BICUBIC, reducing_gap=1)\n",
    "    arr = np.array(pil_image.convert(\"RGB\"))\n",
    "    arr = arr.astype(np.float16) / 127.5 - 1\n",
    "    arr = np.transpose(arr, [2, 0, 1])\n",
    "    image = torch.from_numpy(arr).unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "\n",
    "class NewKandinskyPipeline(KandinskyV22Pipeline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet: UNet2DConditionModel,\n",
    "        scheduler: Union[DDIMScheduler, DDPMScheduler],\n",
    "        movq: VQModel,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            movq=movq,\n",
    "        )\n",
    "        self.forward_diffusion = partial(self.backward_diffusion, reverse_process=True)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def get_image_latents(self, image, sample=False, rng_generator=None):\n",
    "        encoding_dist = self.movq.encode(image).latents\n",
    "        # if sample:\n",
    "        #     encoding = encoding_dist.sample(generator=rng_generator)\n",
    "        # else:\n",
    "        #     encoding = encoding_dist.mode()\n",
    "        latents = encoding_dist #* 0.18215\n",
    "        return latents\n",
    "\n",
    "    def get_timesteps(self, num_inference_steps, strength, device):\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "        t_start = max(num_inference_steps - init_timestep, 0)\n",
    "        timesteps = self.scheduler.timesteps[t_start:]\n",
    "\n",
    "        return timesteps, num_inference_steps - t_start\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def backward_diffusion(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]],\n",
    "        image_embeds: Union[torch.FloatTensor, List[torch.FloatTensor]],\n",
    "        negative_image_embeds: Union[torch.FloatTensor, List[torch.FloatTensor]],\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        height: int = 512,\n",
    "        width: int = 512,\n",
    "        num_inference_steps: int = 100,\n",
    "        guidance_scale: float = 4.0,\n",
    "        num_images_per_prompt: int = 1,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "        callback_steps: int = 1,\n",
    "        return_dict: bool = True,\n",
    "        reverse_process: bool = False,\n",
    "        strength:float =0.3,\n",
    "    ):\n",
    "\n",
    "        if isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        batch_size = batch_size * num_images_per_prompt\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "        # prompt_embeds, text_encoder_hidden_states, _ = self._encode_prompt(\n",
    "        #     prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "        # )\n",
    "\n",
    "        if isinstance(image_embeds, list):\n",
    "            image_embeds = torch.cat(image_embeds, dim=0)\n",
    "        if isinstance(negative_image_embeds, list):\n",
    "            negative_image_embeds = torch.cat(negative_image_embeds, dim=0)\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            image_embeds = image_embeds.repeat_interleave(num_images_per_prompt, dim=0)\n",
    "            negative_image_embeds = negative_image_embeds.repeat_interleave(num_images_per_prompt, dim=0)\n",
    "\n",
    "            image_embeds = torch.cat([negative_image_embeds, image_embeds], dim=0).to(\n",
    "                 device=device, dtype=torch.float16\n",
    "            )\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps_tensor = self.scheduler.timesteps\n",
    "        timesteps_tensor, num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n",
    "\n",
    "        num_channels_latents = self.unet.config.in_channels\n",
    "\n",
    "        height, width = get_new_h_w(height, width, self.movq_scale_factor)\n",
    "\n",
    "        # create initial latent\n",
    "        latents = self.prepare_latents(\n",
    "            (batch_size, num_channels_latents, height, width),\n",
    "            torch.float16,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "            self.scheduler,\n",
    "        )\n",
    "\n",
    "        for i, t in enumerate(self.progress_bar(timesteps_tensor if not reverse_process else reversed(timesteps_tensor))):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "\n",
    "            added_cond_kwargs = {\"image_embeds\": image_embeds}\n",
    "            noise_pred = self.unet(\n",
    "                sample=latent_model_input,\n",
    "                timestep=t,\n",
    "                encoder_hidden_states=None,\n",
    "                added_cond_kwargs=added_cond_kwargs,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            if do_classifier_free_guidance:\n",
    "                noise_pred, variance_pred = noise_pred.split(latents.shape[1], dim=1)\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                _, variance_pred_text = variance_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "                noise_pred = torch.cat([noise_pred, variance_pred_text], dim=1)\n",
    "\n",
    "            if not (\n",
    "                hasattr(self.scheduler.config, \"variance_type\")\n",
    "                and self.scheduler.config.variance_type in [\"learned\", \"learned_range\"]\n",
    "            ):\n",
    "                noise_pred, _ = noise_pred.split(latents.shape[1], dim=1)\n",
    "\n",
    "            prev_timestep = (\n",
    "                t\n",
    "                - self.scheduler.config.num_train_timesteps\n",
    "                // self.scheduler.num_inference_steps\n",
    "            )\n",
    "            # ddim \n",
    "            alpha_prod_t = self.scheduler.alphas_cumprod[t]\n",
    "            alpha_prod_t_prev = (\n",
    "                self.scheduler.alphas_cumprod[prev_timestep]\n",
    "                if prev_timestep >= 0\n",
    "                else self.scheduler.final_alpha_cumprod\n",
    "            )\n",
    "            \n",
    "            if reverse_process:\n",
    "                alpha_prod_t, alpha_prod_t_prev = alpha_prod_t_prev, alpha_prod_t\n",
    "                latents = backward_ddim(\n",
    "                    x_t=latents,\n",
    "                    alpha_t=alpha_prod_t,\n",
    "                    alpha_tm1=alpha_prod_t_prev,\n",
    "                    eps_xt=noise_pred,\n",
    "                )\n",
    "            else:\n",
    "                latents = self.scheduler.step(\n",
    "                    noise_pred,\n",
    "                    t,\n",
    "                    latents,\n",
    "                    generator=generator,\n",
    "                ).prev_sample\n",
    "                \n",
    "\n",
    "            if callback is not None and i % callback_steps == 0:\n",
    "                step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                callback(step_idx, t, latents)\n",
    "        \n",
    "        return latents \n",
    "        \n",
    "    @torch.inference_mode()\n",
    "    def decode_image(self, latents: torch.FloatTensor, **kwargs) -> List[\"PIL_IMAGE\"]:\n",
    "        return self.movq.decode(latents, force_not_quantize=True)[\"sample\"]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def torch_to_numpy(self, image) -> List[\"PIL_IMAGE\"]:\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models import PriorTransformer\n",
    "from transformers import CLIPImageProcessor, CLIPTextModelWithProjection, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from diffusers.schedulers import UnCLIPScheduler\n",
    "from diffusers.utils import BaseOutput\n",
    "\n",
    "class KandinskyPriorPipelineOutput(BaseOutput):\n",
    "    \"\"\"\n",
    "    Output class for KandinskyPriorPipeline.\n",
    "\n",
    "    Args:\n",
    "        image_embeds (`torch.FloatTensor`)\n",
    "            clip image embeddings for text prompt\n",
    "        negative_image_embeds (`List[PIL.Image.Image]` or `np.ndarray`)\n",
    "            clip image embeddings for unconditional tokens\n",
    "    \"\"\"\n",
    "\n",
    "    image_embeds: Union[torch.FloatTensor, np.ndarray]\n",
    "    negative_image_embeds: Union[torch.FloatTensor, np.ndarray]\n",
    "    \n",
    "class NewKandinskyPriorPipeline(KandinskyV22PriorPipeline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        prior: PriorTransformer,\n",
    "        image_encoder: CLIPVisionModelWithProjection,\n",
    "        text_encoder: CLIPTextModelWithProjection,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        scheduler: UnCLIPScheduler,\n",
    "        image_processor: CLIPImageProcessor,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            prior=prior,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            scheduler=scheduler,\n",
    "            image_encoder=image_encoder,\n",
    "            image_processor=image_processor,\n",
    "        )\n",
    "\n",
    "    def _encode_image(\n",
    "        self,\n",
    "        image: Union[torch.Tensor, List[PIL.Image.Image]],\n",
    "        device,\n",
    "        num_images_per_prompt,\n",
    "    ):\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = self.image_processor(image, return_tensors=\"pt\").pixel_values.to(\n",
    "                dtype=self.image_encoder.dtype, device=device\n",
    "            )\n",
    "\n",
    "        image_emb = self.image_encoder(image)[\"image_embeds\"]  # B, D\n",
    "        image_emb = image_emb.repeat_interleave(num_images_per_prompt, dim=0)\n",
    "        image_emb.to(device=device)\n",
    "\n",
    "        return image_emb\n",
    "\n",
    "    def prepare_latents_new(self, emb, timestep, batch_size, num_images_per_prompt, dtype, device, generator=None):\n",
    "        emb = emb.to(device=device, dtype=dtype)\n",
    "\n",
    "        batch_size = batch_size * num_images_per_prompt\n",
    "\n",
    "        init_latents = emb\n",
    "\n",
    "        if batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] == 0:\n",
    "            additional_image_per_prompt = batch_size // init_latents.shape[0]\n",
    "            init_latents = torch.cat([init_latents] * additional_image_per_prompt, dim=0)\n",
    "        elif batch_size > init_latents.shape[0] and batch_size % init_latents.shape[0] != 0:\n",
    "            raise ValueError(\n",
    "                f\"Cannot duplicate `image` of batch size {init_latents.shape[0]} to {batch_size} text prompts.\"\n",
    "            )\n",
    "        else:\n",
    "            init_latents = torch.cat([init_latents], dim=0)\n",
    "\n",
    "        shape = init_latents.shape\n",
    "        noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
    "\n",
    "        # get latents\n",
    "        init_latents = self.scheduler.add_noise(init_latents, noise, timestep)\n",
    "        latents = init_latents\n",
    "\n",
    "        return latents\n",
    "\n",
    "    def get_timesteps(self, num_inference_steps, strength, device):\n",
    "        # get the original timestep using init_timestep\n",
    "        init_timestep = min(int(num_inference_steps * strength), num_inference_steps)\n",
    "\n",
    "        t_start = max(num_inference_steps - init_timestep, 0)\n",
    "        timesteps = self.scheduler.timesteps[t_start:]\n",
    "\n",
    "        return timesteps, num_inference_steps - t_start\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def new_forward(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]],\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: int = 1,\n",
    "        num_inference_steps: int = 25,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        guidance_scale: float = 4.0,\n",
    "        output_type: Optional[str] = \"pt\",\n",
    "        return_dict: bool = True,\n",
    "        image_pil = None,\n",
    "        strength: float = 0.3,\n",
    "    ):\n",
    "\n",
    "        if isinstance(prompt, str):\n",
    "            prompt = [prompt]\n",
    "        elif not isinstance(prompt, list):\n",
    "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
    "\n",
    "        if isinstance(negative_prompt, str):\n",
    "            negative_prompt = [negative_prompt]\n",
    "        elif not isinstance(negative_prompt, list) and negative_prompt is not None:\n",
    "            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n",
    "\n",
    "        # if the negative prompt is defined we double the batch size to\n",
    "        # directly retrieve the negative prompt embedding\n",
    "        if negative_prompt is not None:\n",
    "            prompt = prompt + negative_prompt\n",
    "            negative_prompt = 2 * negative_prompt\n",
    "\n",
    "        device = self._execution_device\n",
    "\n",
    "        batch_size = len(prompt)\n",
    "        batch_size = batch_size * num_images_per_prompt\n",
    "\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "        prompt_embeds, text_encoder_hidden_states, text_mask = self._encode_prompt(\n",
    "            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "        )\n",
    "\n",
    "        image_embeds = self._encode_image(image_pil, device, num_images_per_prompt)\n",
    "\n",
    "        # prior\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        prior_timesteps_tensor = self.scheduler.timesteps\n",
    "\n",
    "        embedding_dim = self.prior.config.embedding_dim\n",
    "\n",
    "        latents = image_embeds\n",
    "        prior_timesteps_tensor, num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n",
    "        latent_timestep = prior_timesteps_tensor[:1].repeat(batch_size)\n",
    "        latents = self.prepare_latents_new(\n",
    "            latents,\n",
    "            latent_timestep,\n",
    "            batch_size // num_images_per_prompt,\n",
    "            num_images_per_prompt,\n",
    "            prompt_embeds.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "        )\n",
    "\n",
    "        for i, t in enumerate(self.progress_bar(prior_timesteps_tensor)):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "\n",
    "            predicted_image_embedding = self.prior(\n",
    "                latent_model_input,\n",
    "                timestep=t,\n",
    "                proj_embedding=prompt_embeds,\n",
    "                encoder_hidden_states=text_encoder_hidden_states,\n",
    "                attention_mask=text_mask,\n",
    "            ).predicted_image_embedding\n",
    "\n",
    "            if do_classifier_free_guidance:\n",
    "                predicted_image_embedding_uncond, predicted_image_embedding_text = predicted_image_embedding.chunk(2)\n",
    "                predicted_image_embedding = predicted_image_embedding_uncond + guidance_scale * (\n",
    "                    predicted_image_embedding_text - predicted_image_embedding_uncond\n",
    "                )\n",
    "\n",
    "            if i + 1 == prior_timesteps_tensor.shape[0]:\n",
    "                prev_timestep = None\n",
    "            else:\n",
    "                prev_timestep = prior_timesteps_tensor[i + 1]\n",
    "\n",
    "            latents = self.scheduler.step(\n",
    "                predicted_image_embedding,\n",
    "                timestep=t,\n",
    "                sample=latents,\n",
    "                generator=generator,\n",
    "                prev_timestep=prev_timestep,\n",
    "            ).prev_sample\n",
    "\n",
    "        latents = self.prior.post_process_latents(latents)\n",
    "\n",
    "        image_embeddings = latents\n",
    "\n",
    "        # if negative prompt has been defined, we retrieve split the image embedding into two\n",
    "        if negative_prompt is None:\n",
    "            zero_embeds = self.get_zero_embed(latents.shape[0], device=latents.device)\n",
    "\n",
    "        else:\n",
    "            image_embeddings, zero_embeds = image_embeddings.chunk(2)\n",
    "\n",
    "            if hasattr(self, \"final_offload_hook\") and self.final_offload_hook is not None:\n",
    "                self.prior_hook.offload()\n",
    "\n",
    "        if output_type not in [\"pt\", \"np\"]:\n",
    "            raise ValueError(f\"Only the output types `pt` and `np` are supported not output_type={output_type}\")\n",
    "\n",
    "        if output_type == \"np\":\n",
    "            image_embeddings = image_embeddings.cpu().numpy()\n",
    "            zero_embeds = zero_embeds.cpu().numpy()\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image_embeddings, zero_embeds)\n",
    "\n",
    "        return KandinskyPriorPipelineOutput(image_embeds=image_embeddings, negative_image_embeds=zero_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model_index.json: 100%|██████████████████████| 250/250 [00:00<00:00, 2.31MB/s]\n",
      "Fetching 6 files:   0%|                                             | 0/6 [00:00<?, ?it/s]\n",
      "Downloading unet/config.json: 100%|██████████████████| 1.67k/1.67k [00:00<00:00, 18.7MB/s]\u001b[A\n",
      "\n",
      "Downloading movq/config.json: 100%|██████████████████████| 660/660 [00:00<00:00, 8.54MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)cheduler_config.json: 100%|███████████████| 317/317 [00:00<00:00, 3.97MB/s]\u001b[A\n",
      "Fetching 6 files:  33%|████████████▎                        | 2/6 [00:00<00:00,  6.94it/s]\n",
      "Downloading (…)ch_model.safetensors:   0%|                     | 0.00/271M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   0%|                    | 0.00/5.01G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors:   4%|▍           | 10.5M/271M [00:00<00:03, 73.6MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   0%|            | 21.0M/5.01G [00:00<00:35, 142MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors:   8%|▉           | 21.0M/271M [00:00<00:03, 81.9MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   1%|▏           | 52.4M/5.01G [00:00<00:23, 209MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   2%|▏           | 83.9M/5.01G [00:00<00:20, 239MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors:  15%|█▊          | 41.9M/271M [00:00<00:02, 99.3MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   2%|▎            | 115M/5.01G [00:00<00:19, 249MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors:  23%|███          | 62.9M/271M [00:00<00:01, 110MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   3%|▍            | 147M/5.01G [00:00<00:19, 246MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors:  31%|████         | 83.9M/271M [00:00<00:01, 121MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   4%|▍            | 178M/5.01G [00:00<00:20, 233MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   4%|▌            | 210M/5.01G [00:00<00:20, 237MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors:  39%|█████▍        | 105M/271M [00:00<00:01, 124MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   5%|▋            | 241M/5.01G [00:01<00:21, 224MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors:  46%|██████▍       | 126M/271M [00:01<00:01, 130MB/s]\u001b[A\n",
      "Downloading (…)ch_model.safetensors:  54%|███████▌      | 147M/271M [00:01<00:00, 134MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   5%|▋            | 273M/5.01G [00:01<00:22, 212MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   6%|▊            | 304M/5.01G [00:01<00:21, 217MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors:  62%|████████▋     | 168M/271M [00:01<00:00, 135MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   7%|▊            | 336M/5.01G [00:01<00:21, 222MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors:  70%|█████████▋    | 189M/271M [00:01<00:00, 137MB/s]\u001b[A\n",
      "Downloading (…)ch_model.safetensors:  77%|██████████▊   | 210M/271M [00:01<00:00, 139MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   7%|▉            | 367M/5.01G [00:01<00:22, 206MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   8%|█            | 388M/5.01G [00:01<00:23, 196MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors:  85%|███████████▉  | 231M/271M [00:01<00:00, 139MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   8%|█            | 409M/5.01G [00:01<00:23, 194MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors:  93%|████████████▉ | 252M/271M [00:01<00:00, 140MB/s]\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   9%|█            | 430M/5.01G [00:02<00:23, 198MB/s]\u001b[A\u001b[A\n",
      "Downloading (…)ch_model.safetensors: 100%|██████████████| 271M/271M [00:02<00:00, 128MB/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading (…)ch_model.safetensors:   9%|█▏           | 451M/5.01G [00:02<00:23, 194MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  10%|█▎           | 482M/5.01G [00:02<00:20, 219MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  10%|█▎           | 524M/5.01G [00:02<00:17, 259MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  11%|█▍           | 566M/5.01G [00:02<00:15, 289MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  12%|█▌           | 608M/5.01G [00:02<00:14, 311MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  13%|█▋           | 650M/5.01G [00:02<00:13, 317MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Fetching 6 files:  50%|██████████████████▌                  | 3/6 [00:03<00:04,  1.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  15%|█▉           | 734M/5.01G [00:02<00:12, 331MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  15%|██           | 776M/5.01G [00:03<00:12, 333MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  16%|██           | 818M/5.01G [00:03<00:12, 333MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  17%|██▏          | 860M/5.01G [00:03<00:12, 330MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  18%|██▎          | 902M/5.01G [00:03<00:12, 338MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  19%|██▍          | 944M/5.01G [00:03<00:11, 346MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  20%|██▌          | 986M/5.01G [00:03<00:11, 343MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  21%|██▍         | 1.03G/5.01G [00:03<00:11, 357MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  21%|██▌         | 1.07G/5.01G [00:03<00:10, 368MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  22%|██▋         | 1.11G/5.01G [00:04<00:10, 374MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  23%|██▊         | 1.15G/5.01G [00:04<00:10, 380MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  24%|██▊         | 1.20G/5.01G [00:04<00:09, 388MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  25%|██▉         | 1.24G/5.01G [00:04<00:09, 391MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  26%|███         | 1.28G/5.01G [00:04<00:09, 395MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  26%|███▏        | 1.32G/5.01G [00:04<00:09, 394MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  27%|███▎        | 1.36G/5.01G [00:04<00:09, 400MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  28%|███▎        | 1.41G/5.01G [00:04<00:09, 396MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  29%|███▍        | 1.45G/5.01G [00:04<00:09, 389MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  30%|███▌        | 1.49G/5.01G [00:04<00:09, 389MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  31%|███▋        | 1.53G/5.01G [00:05<00:09, 380MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  31%|███▊        | 1.57G/5.01G [00:05<00:09, 357MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  32%|███▊        | 1.61G/5.01G [00:05<00:09, 341MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  33%|███▉        | 1.66G/5.01G [00:05<00:10, 325MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  34%|████        | 1.70G/5.01G [00:05<00:10, 320MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  35%|████▏       | 1.74G/5.01G [00:05<00:10, 322MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  36%|████▎       | 1.78G/5.01G [00:05<00:09, 342MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  36%|████▎       | 1.82G/5.01G [00:05<00:09, 346MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  37%|████▍       | 1.87G/5.01G [00:06<00:09, 348MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  38%|████▌       | 1.91G/5.01G [00:06<00:09, 335MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  39%|████▋       | 1.95G/5.01G [00:06<00:09, 326MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  40%|████▊       | 1.99G/5.01G [00:06<00:09, 319MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  41%|████▊       | 2.03G/5.01G [00:06<00:09, 329MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  41%|████▉       | 2.08G/5.01G [00:06<00:08, 342MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  42%|█████       | 2.12G/5.01G [00:06<00:08, 354MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  43%|█████▏      | 2.16G/5.01G [00:06<00:07, 366MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  44%|█████▎      | 2.21G/5.01G [00:07<00:07, 390MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  45%|█████▍      | 2.25G/5.01G [00:07<00:06, 395MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  46%|█████▌      | 2.31G/5.01G [00:07<00:06, 405MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  47%|█████▌      | 2.35G/5.01G [00:07<00:06, 398MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  48%|█████▋      | 2.39G/5.01G [00:07<00:06, 399MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  49%|█████▊      | 2.43G/5.01G [00:07<00:06, 391MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  49%|█████▉      | 2.47G/5.01G [00:07<00:06, 365MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  50%|██████      | 2.52G/5.01G [00:07<00:07, 347MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  51%|██████▏     | 2.56G/5.01G [00:08<00:07, 333MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  52%|██████▏     | 2.60G/5.01G [00:08<00:07, 320MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  53%|██████▎     | 2.64G/5.01G [00:08<00:07, 318MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  54%|██████▍     | 2.68G/5.01G [00:08<00:07, 325MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  54%|██████▌     | 2.73G/5.01G [00:08<00:07, 326MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  55%|██████▋     | 2.77G/5.01G [00:08<00:06, 329MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  56%|██████▋     | 2.81G/5.01G [00:08<00:07, 315MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  57%|██████▊     | 2.85G/5.01G [00:08<00:06, 318MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  58%|██████▉     | 2.89G/5.01G [00:09<00:06, 315MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  59%|███████     | 2.94G/5.01G [00:09<00:10, 205MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  59%|███████     | 2.97G/5.01G [00:09<00:09, 218MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  60%|███████▏    | 3.00G/5.01G [00:09<00:08, 233MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  60%|███████▎    | 3.03G/5.01G [00:09<00:07, 249MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  61%|███████▎    | 3.07G/5.01G [00:09<00:07, 276MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  62%|███████▍    | 3.11G/5.01G [00:10<00:09, 202MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  63%|███████▌    | 3.15G/5.01G [00:10<00:09, 196MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  63%|███████▌    | 3.18G/5.01G [00:10<00:08, 217MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  64%|███████▋    | 3.22G/5.01G [00:10<00:07, 256MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  65%|███████▊    | 3.26G/5.01G [00:10<00:06, 291MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  66%|███████▉    | 3.31G/5.01G [00:10<00:05, 331MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  67%|████████    | 3.36G/5.01G [00:10<00:04, 343MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  68%|████████▏   | 3.41G/5.01G [00:11<00:04, 373MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  69%|████████▎   | 3.45G/5.01G [00:11<00:04, 374MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  70%|████████▎   | 3.49G/5.01G [00:11<00:04, 352MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  71%|████████▍   | 3.53G/5.01G [00:11<00:04, 336MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  71%|████████▌   | 3.58G/5.01G [00:11<00:04, 340MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  72%|████████▋   | 3.62G/5.01G [00:11<00:03, 358MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  73%|████████▊   | 3.66G/5.01G [00:11<00:03, 372MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  74%|████████▊   | 3.70G/5.01G [00:11<00:03, 364MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  75%|████████▉   | 3.74G/5.01G [00:12<00:03, 354MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  76%|█████████   | 3.79G/5.01G [00:12<00:03, 346MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  76%|█████████▏  | 3.83G/5.01G [00:12<00:03, 345MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  77%|█████████▎  | 3.87G/5.01G [00:12<00:03, 340MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  78%|█████████▎  | 3.91G/5.01G [00:12<00:03, 341MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  79%|█████████▍  | 3.95G/5.01G [00:12<00:03, 336MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  80%|█████████▌  | 4.00G/5.01G [00:12<00:02, 353MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  81%|█████████▋  | 4.04G/5.01G [00:12<00:02, 368MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  82%|█████████▊  | 4.09G/5.01G [00:12<00:02, 393MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  82%|█████████▉  | 4.13G/5.01G [00:13<00:02, 398MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  83%|██████████  | 4.18G/5.01G [00:13<00:01, 415MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  84%|██████████  | 4.23G/5.01G [00:13<00:01, 413MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  85%|██████████▏ | 4.27G/5.01G [00:13<00:01, 408MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  86%|██████████▎ | 4.31G/5.01G [00:13<00:01, 380MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  87%|██████████▍ | 4.35G/5.01G [00:13<00:01, 366MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  88%|██████████▌ | 4.39G/5.01G [00:13<00:01, 345MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  88%|██████████▌ | 4.44G/5.01G [00:13<00:01, 347MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  90%|██████████▋ | 4.49G/5.01G [00:14<00:01, 372MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  90%|██████████▊ | 4.53G/5.01G [00:14<00:01, 377MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  91%|██████████▉ | 4.57G/5.01G [00:14<00:01, 382MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  92%|███████████ | 4.61G/5.01G [00:14<00:01, 384MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  93%|███████████▏| 4.66G/5.01G [00:14<00:00, 389MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  94%|███████████▏| 4.70G/5.01G [00:14<00:00, 375MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  95%|███████████▎| 4.74G/5.01G [00:14<00:00, 359MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  95%|███████████▍| 4.78G/5.01G [00:14<00:00, 358MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  96%|███████████▌| 4.82G/5.01G [00:14<00:00, 363MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  97%|███████████▋| 4.87G/5.01G [00:15<00:00, 349MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  98%|███████████▋| 4.91G/5.01G [00:15<00:00, 336MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors:  99%|███████████▊| 4.95G/5.01G [00:15<00:00, 333MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)ch_model.safetensors: 100%|████████████| 5.01G/5.01G [00:15<00:00, 323MB/s]\u001b[A\u001b[A\n",
      "Fetching 6 files: 100%|█████████████████████████████████████| 6/6 [00:26<00:00,  4.43s/it]\n",
      "Loading pipeline components...: 100%|███████████████████████| 3/3 [00:12<00:00,  4.06s/it]\n",
      "/home/nkusumba/.conda/envs/nlp-project/lib/python3.10/site-packages/diffusers/configuration_utils.py:239: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "pipe = NewKandinskyPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\",\n",
    "                                            torch_dtype=torch.float16\n",
    "                                           )\n",
    "pipe.scheduler = DDIMScheduler.from_config(\"kandinsky-community/kandinsky-2-2-decoder\", subfolder=\"scheduler\")\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path, target_size=512):\n",
    "    \"\"\"Load an image, resize and output -1..1\"\"\"\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    image = prepare_image(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|███████████████████████████| 570/570 [00:00<00:00, 3.48MB/s]\n",
      "Downloading model.safetensors: 100%|████████████████████| 440M/440M [00:01<00:00, 379MB/s]\n",
      "Loading pipeline components...: 100%|███████████████████████| 3/3 [00:01<00:00,  2.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T2IModel(\n",
       "  (text_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (vision_model): CLIPVisionModelWithProjection(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1664, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(257, 1664)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-47): 48 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              (v_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              (q_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "              (out_proj): Linear(in_features=1664, out_features=1664, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1664, out_features=8192, bias=True)\n",
       "              (fc2): Linear(in_features=8192, out_features=1664, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): Linear(in_features=1664, out_features=1280, bias=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=2048, out_features=1280, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CLIPTextModelWithProjection, CLIPVisionModelWithProjection, BertModel\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "import os\n",
    "\n",
    "\n",
    "class T2IModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(T2IModel, self).__init__()\n",
    "        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.vision_model = CLIPVisionModelWithProjection.from_pretrained(\n",
    "            \"kandinsky-community/kandinsky-2-2-prior\", subfolder=\"image_encoder\"\n",
    "        )\n",
    "        # Adjust the input features of the FC layer to the combined size of text and vision outputs\n",
    "        self.fc = nn.Linear(self.text_model.config.hidden_size + self.vision_model.config.projection_dim, 1280)\n",
    "        self.pipe = KandinskyV22Pipeline.from_pretrained(\n",
    "            \"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16\n",
    "        )\n",
    "\n",
    "    def initialize_optimizer(self):\n",
    "        params = (\n",
    "            list(self.fc.parameters())\n",
    "        )\n",
    "        optimizer = AdamW(params, lr=1e-4)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, input_imgs, input_txt, attention_mask=None):\n",
    "        text_outputs = self.text_model(input_txt, attention_mask=attention_mask)\n",
    "        text_embeds = text_outputs.last_hidden_state[:, 0, :]  # Use the representation of the [CLS] token\n",
    "\n",
    "        vision_outputs = self.vision_model(input_imgs)\n",
    "        vision_embeds = vision_outputs.image_embeds\n",
    "\n",
    "        combined_embeds = torch.cat((vision_embeds, text_embeds), dim=1)\n",
    "        x = self.fc(combined_embeds)\n",
    "        return x\n",
    "\n",
    "    def output_embedding(self, target_images):\n",
    "        target_image_output = self.vision_model(target_images)\n",
    "        target_image_embeds = target_image_output.image_embeds\n",
    "        return target_image_embeds\n",
    "\n",
    "    def custom_loss(self, output_embeddings, target_embeddings):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        loss = mse_loss(output_embeddings, target_embeddings)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def save_model(self, output_dir=\"../model_save/\", filename=\"model_checkpoint.pt\"):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        file_path = output_dir + filename\n",
    "        print(\"Saving model to %s\" % file_path)\n",
    "\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "\n",
    "    def get_cos(self, input1, input2):\n",
    "        cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        similarity = cos(input1, input2)\n",
    "        avg = torch.sum(similarity) / len(similarity)\n",
    "        return avg\n",
    "\n",
    "    def metrics(self, input1, input2):\n",
    "        cos = self.get_cos(input1, input2)\n",
    "        return [cos]\n",
    "\n",
    "    def visualization(self, input_img, instruction, instruction_attention_mask, filename, negative_instruction=None, negative_instruction_attention_mask=None):\n",
    "        # Generate output embeddings with the provided attention mask\n",
    "        output_embeddings = self.forward(input_img, instruction, attention_mask=instruction_attention_mask)\n",
    "\n",
    "        # Handle the negative instruction if provided\n",
    "        neg_image_embed = None\n",
    "        if negative_instruction is not None and negative_instruction_attention_mask is not None:\n",
    "            neg_image_embed = self.forward(input_img, negative_instruction, attention_mask=negative_instruction_attention_mask)\n",
    "        else:\n",
    "            # If no negative instruction is provided, we'll use a tensor of zeros as a placeholder\n",
    "            neg_image_embed = torch.zeros_like(output_embeddings)\n",
    "\n",
    "        # Initialize the pipeline for the Kandinsky V2.2 decoder\n",
    "        \n",
    "        self.pipe.to(device)  # Make sure 'self.device' is defined in your model class\n",
    "\n",
    "        # Generate the image using the pipeline\n",
    "        image = self.pipe(\n",
    "            image_embeds=output_embeddings,\n",
    "            negative_image_embeds=neg_image_embed,\n",
    "            height=768,\n",
    "            width=768,\n",
    "            num_inference_steps=100,\n",
    "        ).images\n",
    "\n",
    "        # Save the generated image\n",
    "        image[0].save(filename)\n",
    "\n",
    "model = T2IModel()\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_model_from_checkpoint(model, checkpoint_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load a PyTorch model from a saved checkpoint.\n",
    "    \n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The model architecture (untrained).\n",
    "    - checkpoint_path (str): Path to the saved model checkpoint (.pth file).\n",
    "    - device (str): Device to which the model should be loaded ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Model populated with the loaded weights.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the model state dictionary from the specified path\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Move the model to the desired device\n",
    "    model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Usage\n",
    "loaded_model = load_model_from_checkpoint(model, '/scratch/nkusumba/magicbrush_kadinsky_bert_imagewithinstruction_10epochs_full_v1.pth', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def create_black_image(width, height):\n",
    "    # Create a black image using PIL\n",
    "    return Image.new(\"RGB\", (width, height), (0, 0, 0))\n",
    "\n",
    "def get_black_image():\n",
    "    # Create and return a black image\n",
    "    black_image = Image.new(\"RGB\", (224, 224), (0, 0, 0))  # Adjust size as needed\n",
    "    return black_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoProcessor\n",
    "from torchvision import transforms\n",
    "def get_edited_image(img_path, tokenizer, processor, alternate_prompt):\n",
    "    empty_prompt=\"\"\n",
    "    img = load_img(img_path, 512).to(device,\n",
    "                                     dtype = torch.float16\n",
    "                                    )\n",
    "    image_latents = pipe.get_image_latents(img)\n",
    "    \n",
    "    # Convert image to tensor and ensure consistency in data type\n",
    "    img = Image.open(img_path)\n",
    "    input_image = processor(images=img, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "    black_image = processor(images=get_black_image(), return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "\n",
    "    # Tokenize the prompts\n",
    "    inputs = tokenizer(empty_prompt, return_tensors=\"pt\").to(device)\n",
    "    inputs_alternate = tokenizer(alternate_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    image_emb_alternate = loaded_model(input_image, inputs_alternate[\"input_ids\"]).to(dtype=torch.float16)\n",
    "    zero_image_emb_alternate = loaded_model.vision_model(black_image).image_embeds.to(dtype=torch.float16)\n",
    "    \n",
    "    zero_image_emb_main = zero_image_emb_alternate\n",
    "    image_emb_main = loaded_model.vision_model(input_image).image_embeds.to(dtype=torch.float16)\n",
    "\n",
    "    \n",
    "    reversed_latents = pipe.forward_diffusion(\n",
    "        \"\",\n",
    "        image_embeds=image_emb_main,\n",
    "        negative_image_embeds=zero_image_emb_main,\n",
    "        guidance_scale=1,\n",
    "        num_inference_steps=100,\n",
    "        latents=image_latents,\n",
    "        strength=1.5,\n",
    "    )\n",
    "\n",
    "    alternate_latents = pipe.backward_diffusion(\n",
    "        \"\",\n",
    "        image_embeds=image_emb_alternate,\n",
    "        negative_image_embeds=zero_image_emb_main,\n",
    "        guidance_scale=1,\n",
    "        num_inference_steps=100,\n",
    "        latents=reversed_latents,\n",
    "        strength=1.5,\n",
    "    )\n",
    "\n",
    "    x = pipe.decode_image(alternate_latents)\n",
    "    x = pipe.torch_to_numpy(x)\n",
    "    return pipe.numpy_to_pil(x)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Evaluation\n",
      "Processing 1th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.62it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 31.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 1th image\n",
      "Processing 2th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 31.27it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 33.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 2th image\n",
      "Processing 3th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 32.25it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 31.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 3th image\n",
      "Processing 4th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.27it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 4th image\n",
      "Processing 5th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.29it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 5th image\n",
      "Processing 6th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.09it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 6th image\n",
      "Processing 7th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.57it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 7th image\n",
      "Processing 8th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.40it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 8th image\n",
      "Processing 9th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.46it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 9th image\n",
      "Processing 10th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.81it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 10th image\n",
      "Processing 11th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.58it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 32.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 11th image\n",
      "Processing 12th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.55it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 12th image\n",
      "Processing 13th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.41it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 13th image\n",
      "Processing 14th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.61it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 14th image\n",
      "Processing 15th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.36it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 32.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 15th image\n",
      "Processing 16th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.40it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 16th image\n",
      "Processing 17th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.11it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 28.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 17th image\n",
      "Processing 18th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.02it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 18th image\n",
      "Processing 19th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 34.04it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 19th image\n",
      "Processing 20th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 29.19it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 25.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 20th image\n",
      "Processing 21th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 28.70it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 27.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 21th image\n",
      "Processing 22th image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 30.10it/s]\n",
      "100%|███████████████████████████████████████████████████| 100/100 [00:03<00:00, 29.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 22th image\n",
      "Processing 23th image\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "file_path = '/scratch/nkusumba/test/edit_sessions.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Printing each key and its corresponding value\n",
    "dic = {}\n",
    "for key, value in json_data.items():\n",
    "    dic[key] = value[0]['instruction']\n",
    "\n",
    "images_path = '/scratch/nkusumba/test/images/'\n",
    "os.makedirs('/scratch/nkusumba/test/outputs/', exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "print('Started Evaluation')\n",
    "count = -1\n",
    "dir_name = []\n",
    "for dirpath, dirname, filenames in os.walk(images_path):\n",
    "    if count == -1:\n",
    "        count = 0\n",
    "        dir_name = dirname\n",
    "    if count == 100:\n",
    "        print('Process done!!!')\n",
    "        break\n",
    "    input_path = ''\n",
    "    output_path = ''\n",
    "    for file in filenames:\n",
    "        filepath = os.path.join(dirpath, file)\n",
    "        if filepath.endswith('input.png'):\n",
    "            input_path = filepath\n",
    "        elif filepath.endswith('output1.png'):\n",
    "            output_path = filepath\n",
    "    if input_path == '':\n",
    "        continue\n",
    "    print(f'Processing {count+1}th image')\n",
    "    dir = f'/scratch/nkusumba/test/outputs/{count+1}'\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    out_img = Image.open(output_path)\n",
    "    out_img.save(f'{dir}/groundtruth.png')\n",
    "\n",
    "    # Process the image\n",
    "    img = Image.open(input_path)\n",
    "    img.save(f'{dir}/input_image.png')\n",
    "\n",
    "    # Process the instruction\n",
    "    instruction = dic[dir_name[count]]\n",
    "    with open(f'{dir}/instruction.txt', 'w') as f:\n",
    "        f.write(instruction)\n",
    "        \n",
    "    # Visualize the output\n",
    "    out_img = get_edited_image(input_path, tokenizer, processor, instruction)\n",
    "    out_img.save(f'{dir}/output.png')\n",
    "    print(f'Finished processing {count+1}th image')\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "nlp-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
